---
title: "COS-D419 Factor Analysis and Structural Equation Models 2023, Assignment 5"
author: "Rong Guang"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    latex_engine: lualatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F, cache = F, fig.align = 'center')
```

# Read me

The texts that reflect my understanding/questions/doubts have been highlighted in \textcolor{red}{red color}. The texts that describes important steps/results or that corresponds to certain exercise requirement have been highlighted in \textcolor{blue}{blue color}.

```{r, echo = F}
## Exercise 5.1
# 
# Specify and estimate the initial baseline models for the two groups. 
# 
# Present a brief summary of the model fit and make the first step of the modification by including **(exceptionally, at the same time!)** all the four parameters known to be required for improving the model fit of both models.
# 
# Fine-tune the models step by step following the guidelines given in the lecture material, i.e., implement the modifications **(as usually, one change at a time)** testing and studying each step. 
# 
# Present the final baseline models of each group and draw the graphs
# 
# ## Exercise 5.2: 
# 
# **Now we are getting to the point of this Assignment: testing the invariance.** See slide #14.
# 
# *Note:* To continue you have to combine the two data sets into one data set, so that it includes a variable for identifying the two groups.
# 
# As soon as you have the new data, go ahead and specify the *common baseline (configural) model*, and then begin the **invariance testing** by estimating that configural model and studying its output. See below! A lot of ready R code given! :)
# 
# ## Exercise 5.3
# 
# Summarize the whole invariance testing concisely, preferably with a summary table that displays the key results of each phase. 
# 
# **Also write a brief conclusion of the whole testing process using your own words.**
```

# Preparation

## Read in the data set:

```{r}
#install the necessary pakages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(here, 
               expss, 
               tidyverse, 
               lavaan,
               semPlot,
               janitor,
               knitr,  
               stringr,
               labelled,
               ggstatsplot,
               ggcorplot)

library(tidyverse)
library(readr)

#This week's file name
latest.name1 <- "MBIELM1.CSV"
latest.name2 <- "MBISEC1.CSV"
#read in the data
mbi.elm <-  #elementary school
  read_csv(
    file.path(
      here(),
      'data',
      latest.name1
      )
    )

mbi.sec <- #secondary school
  read_csv(
    file.path(
      here(),
      'data',
      latest.name2
      )
    )

```

## Write functions

To control length of reports, codes of fucntions were not showing in the current report. Yet they are available in .rmd report.

### To generate a function for calculating chi square difference was defined.

```{r, echo = F}
chisq_mlm <- function(fit_nested, fit_parent) {
    # scaling correction factors
      c0 <- fitMeasures(fit_nested, "chisq.scaling.factor") %>% as.numeric()
      c1 <- fitMeasures(fit_parent, "chisq.scaling.factor") %>% as.numeric()
    # scaling correction of the difference test
      d0 <- fitMeasures(fit_nested, "df") %>% as.numeric()
      d1 <- fitMeasures(fit_parent, "df") %>% as.numeric()
      cd <- ((d0 * c0) - (d1 * c1))/(d0 - d1)
    # MLM chi-square difference test
      T0 <- fitMeasures(fit_nested, "chisq.scaled") %>% as.numeric()
      T1 <- fitMeasures(fit_parent, "chisq.scaled") %>% as.numeric()
      TRd <- (T0*c0 - T1*c1)/cd
    # degrees of freedom
      df = d0 - d1
    return(c("TR_d" = TRd |> round(3), 
             "df" = df |> round(0), 
             "p_value" = pchisq(TRd, df, lower.tail = FALSE) |> round(3)))
}
```

### to generate CFA results with improved readability

```{r, echo = F}
#goodness of fit indicators for ml
cfa.summary.mlm.a <- function(fit){
  options(scipen = 999)
  cfa.measure <- fitMeasures(fit,    #obtain specified measured.
                            c("chisq.scaled", 
                              "df.scaled", 
                              "pvalue.scaled", 
                              "cfi.scaled", 
                              "tli.scaled",
                              "rmsea.scaled",
                              "rmsea.pvalue.scaled",
                              "srmr_bentler",
                              "chisq.scaling.factor")) 
  names(cfa.measure) <- c("chi square", "df", "p value", "CFI", "TLI", "RMSEA", "RMSEA p value", "SRMR", "CSF")
  #turn named vector to data frame
cfa.tab.a <- cfa.measure %>%  
  tibble(name= names(cfa.measure), value = cfa.measure) %>% # vector to df
  select(Measure = name, Value = value) %>%  #select and rename columns
  mutate(Value = round(as.numeric(Value),3)) 
}
#factor loading
cfa.summary.b <- function(fit, fa.num, item.num, estimator){
  options(scipen = 999)
  #factor loading
  cfa.tab.b <- parameterEstimates(fit, standardized=TRUE) %>% # obtain estimates
  filter(op == "=~") %>%  #select "is measured by" rows
  mutate(Parameter = paste0(lhs, "→", rhs),
         pvalue = case_when(as.numeric(pvalue)<0.001~"<0.001", 
                            as.numeric(pvalue)>=0.001~as.character(pvalue)
                            )
         ) |> 
  select(Parameter,
         Beta=std.all, #std estimates
         SE=se, #standard error
         Z=z, #z statistics
         'p-value'=pvalue #p value
         ) 
  # %>%  
  # kable(digits = 3, #rounded to 3
  #       format="markdown", #Latex markdown
  #       booktabs=TRUE, #Latex booktabs
  #       caption=paste("Factor Loadings for",fa.num,"factor CFA model estimated by ", estimator)) %>% #caption
  # kable_styling(latex_options = "striped") %>% #gray every other row
  # row_spec(0, background = "#9999CC") # color the first row
  # cfa.tab.b
  }

#Variance
cfa.summary.c <- function(fit, fa.num, item.num, estimator){
  options(scipen = 999)
  #Variance
  type <- rep(c("Residual", "Total"), 
            time = c(item.num, fa.num)) #create a new row clarifying types of variance

variance <- parameterEstimates(fit, standardized=TRUE) %>% #obtain estimates
  filter(op == "~~") #select "is correlated with" rows
variance <- variance[1:sum(item.num,fa.num),] #subset 1:18 rows (variance row)
variance <- cbind(type, variance) #add column
cfa.tab.c <- variance |> 
  mutate(pvalue = case_when(as.numeric(pvalue)<0.001~"<0.001", 
                            as.numeric(pvalue)>=0.001~as.character(pvalue)
                            )) |> 
  select(Parameter = type, #select and rename variables
                   Indicator=rhs, #right hand side column
                   B=est, #estimates
                   "Beta*"=std.all,#std estimates
                   SE=se,#standard error
                   Z=z, #z statistics
                   'p-value'=pvalue #p value
                   ) 

# %>% 
#   kable(digits = 3, #rounded
#         format="markdown",  #Latex markdown
#         booktabs=TRUE, #Latex booktabs
#         caption=paste("Variances for", fa.num, "factor model estimated by ", estimator)) %>% #caption
#   kable_styling(latex_options = "striped") %>% # gray every other row
#   row_spec(0, background = "#9999CC") # color the variable row
#   cfa.tab.c
}

#Covariance
cfa.summary.d <- function(fit, fa.num, item.num, estimator){
  options(scipen = 999)
  #covariance
  variance <- parameterEstimates(fit, standardized=TRUE) %>%
  filter(op == "~~")
  covar.num = (fa.num+(fa.num-1))/2
variance <- variance[sum(item.num,fa.num,1):sum(item.num,fa.num,covar.num),]
type <- paste(variance$lhs, "←→", variance$rhs) 
variance <- cbind(type, variance)
rownames(variance) <- NULL
cfa.tab.d <- variance |> 
  mutate(pvalue = case_when(as.numeric(pvalue)<0.001~"<0.001", 
                            as.numeric(pvalue)>=0.001~as.character(pvalue)
                            )) |> 
  select(Parameter=type,
         B=est, 
         Beta=std.all,
         SE=se,
         Z=z, 
         'p-value'=pvalue 
         )
# %>% 
#   kable(digits = 3, 
#         format="markdown", 
#         booktabs=TRUE, 
#         caption=paste("Covariances for", fa.num, 
#                       "factor model estimated by ", 
#                       estimator)) %>% 
#   kable_styling(latex_options = "striped") %>% 
#   row_spec(0, background = "#9999CC")
#   cfa.tab.d
}

```

### Write a function to simplify plotting of merged tables for multi-group fit indicies

```{r, echo=F}
multi.fit.tab <- function(data, title, more.footnote = NULL){
data <- data |> 
  rename(p = 'p value',
         p2 = 'RMSEA p value',
         chi = 'chi square') |> 
  mutate(df = as.numeric(df) |> round(0),
         p = case_when(
           as.numeric(p) < 0.001 ~ "<0.001",
           as.numeric(p) >= 0.001 ~ p
           ),
         p2 = case_when(
           as.numeric(p2) < 0.001 ~ "<0.001",
           as.numeric(p2) >= 0.001 ~ p2
           )
         ) |>
  mutate('Chi square (df, p)' = 
           paste0(chi, "(", df,", ", p, ")"),
         'RMSEA(p)'           = 
           paste0(RMSEA, "(", p2, ")"
                  )
         ) |> 
  select(
    Model,
    'Chi square (df, p)', 
    CFI, TLI,
    'RMSEA(p)', 
    SRMR, 
    'CSF*'= CSF
    ) 
#print the combined table with adjustment of aesthetics
data |> 
  kable(booktabs = T, 
        #format = "markdown", 
        caption = 
          title,
        align = "lrrrrrr"
        ) |> 
  kable_styling(full_width = T) |> 
  footnote(symbol = 
             c("Chi square scaling factor", 
               more.footnote)
           ) |>
  column_spec(1, width = "3.5cm") |> 
  column_spec(2, width = "4cm")|> 
  column_spec(3, width = "1cm")|> 
  column_spec(4, width = "1cm")|> 
  column_spec(5, width = "2.5cm")|> 
  column_spec(6, width = "1cm") |> 
  column_spec(7, width = "1cm") 
}
```

### Write a function to simplify plotting of merged tables for multi-group fit indicies with chi square difference statistics

```{r, echo=F}
delta.fit.tab <- function(data, title, more.footnote = NULL, 
                          compare = "configural",
                          row.correction = 0){
  if (compare == "configural"){
    compare <- "inv1.fit"
  } else {compare <- paste0(compare, ".fit")}
  
  
  data$'ΔChi-square(df,p)*' <- rep(NA, nrow(data))
  for (i in 2:nrow(data)){
    nested <- paste0("inv", sum(i, row.correction), ".fit")
    diff<- as.numeric(chisq_mlm(eval(parse(text = nested)), 
                                eval(parse(text = compare)))[1])
    diff.df <-  as.numeric(chisq_mlm(eval(parse(text = nested)), 
                                     eval(parse(text = compare)))[2])
    diff.p<- as.numeric(chisq_mlm(eval(parse(text = nested)), 
                                  eval(parse(text = compare)))[3])                 
    data$'ΔChi-square(df,p)*'[i] <-  paste0(diff, "(", diff.df,", ", diff.p, ")")
  }
 

data <- data |> 
  rename(p = 'p value',
         p2 = 'RMSEA p value',
         chi = 'chi square') |> 
  mutate(df = as.numeric(df) |> round(0),
         p = case_when(
           as.numeric(p) < 0.001 ~ "<0.001",
           as.numeric(p) >= 0.001 ~ p
           ),
         p2 = case_when(
           as.numeric(p2) < 0.001 ~ "<0.001",
           as.numeric(p2) >= 0.001 ~ p2
           )
         ) |>
  mutate('Chi square (df, p)' = 
           paste0(chi, "(", df,", ", p, ")"),
         'RMSEA(p)'           = 
           paste0(RMSEA, "(", p2, ")"
              )
         ) |> 
  select(
    Model,
    'Chi square (df, p)', 
    'ΔChi-square(df,p)*',
    CFI, TLI,
    'RMSEA(p)', 
    SRMR
    ) 
data[1,3] <- "__"
#print the combined table with adjustment of aesthetics
data |> 
  kable(booktabs = T, 
        #format = "markdown", 
        caption = 
          title,
        align = "lrrrrrr"
        ) |> 
  kable_styling(full_width = T) |> 
  footnote(symbol = 
             c(paste(
               "Chi square differece statistics of model of the row compared to", 
               compare, 
               "model"), 
               more.footnote)
           ) |>
  column_spec(1, width = "2.8cm") |> 
  column_spec(2, width = "3.5cm")|> 
  column_spec(3, width = "3cm")|> 
  column_spec(4, width = "1cm")|> 
  column_spec(5, width = "1cm")|> 
  column_spec(6, width = "2.2cm") |> 
  column_spec(7, width = "1cm") 
}

```

### Write a function to simplify plotting aligned residual variance and co-variance tables

```{r, echo=F}
align.table <- function(data, num.no.header.col, title){

data  |> 
  kable(
    digits = 3,
    booktabs = T,
    #format = "markdown",
    caption = title,
    linesep = ""
    ) |>  
  add_header_above(c(" " = num.no.header.col, 
                     "Elementary level" = 5,
                     "Secondary level" = 5
                     )
                   ) |> 
  kable_styling(
    latex_options = "striped"
  ) |> 
  footnote(
           symbol = c(
             "Un-standardized estimates",
             "Standardized estimates"
                      )
           )
}
```

### Write a function for correlation matrix with numbers

```{r, echo = F}
mymatrix <- function(data, fig.num = 3){
  library(GGally)
ggcorr(data, 
       geom = "blank", 
       label = TRUE, 
       hjust = 0.9, 
       color = "red", 
       face = "bold", 
       method = c("pairwise","pearson"),
       digits = 2,
       size= 2.5,
       label_size = 2.5,
       label_round = 2,
       layout.exp =1) +
  geom_point(size = 7, 
             aes(color = "steelblue", 
                 alpha = abs(coefficient) > 0.3)) +
  scale_alpha_manual(values = c("TRUE" = 0.4, 
                                "FALSE" = 0)) +
    geom_point(size = 8, 
               aes(color = "red", 
                   alpha = abs(coefficient) > 0.6)) +
  scale_alpha_manual(values = c("TRUE" = 0.4, 
                                "FALSE" = 0)) +
  guides(color = FALSE, 
         alpha = FALSE) +
  labs(title = paste("Figure ", fig.num," Pearson correlation matrix of the selected items"),
       caption = 
         " Red circles indicates the absolute of correlation coefficient >= 0.6 
        green circle indicates >= 0.3")+
  theme(plot.title = element_text(size = 12,
                                  face = "bold",
                                  hjust = 0.5),
        plot.caption = element_text(color = "red"))
}

```

### to generate a function for histogram overlapping with density plot

```{r, echo = F}
corr.density <- function(data, fig.num = 1){
  data %>% 
  pivot_longer(everything()) %>%  #longer format
  ggplot(aes(x = value)) + #x axis used variable "value" (a default of pivot)
  geom_histogram(binwidth = 1, aes(y = ..density..), #match ys of density and histogram plots
                 color = "black",  fill = "#9999CC")+  # adjust aesthetics for hist
  geom_density(fill = "pink", alpha = 0.25)+ #adjust aesthetics for density plot
  facet_wrap(~name, scales = "free", ncol =4) + #wrap by name variable
  theme(panel.grid.major = element_blank(), #get rid of the  grids
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "white",#adjust the background
                                        color = "black"),
        strip.background = element_rect(color = "black",#adjust the strips aes
                                        fill = "steelblue"),
        strip.text = element_text(size =8, color = "white"), #adjust strip text
        axis.title.x = element_text(size = 3), #adjust the x text
        axis.title.y = element_text(size = 3), # adjust the y text
        plot.title = element_text(size = 12, 
                                  face = "bold",
                                  hjust = 0.5))+ #adjust the title
  labs(title = paste("Figure ", fig.num," Distribution of selected items")) #title it
  }

```

### to generate a function for violin overlapping with box plot

```{r, echo = F}
violin.box <- function(data, fig.num = 2){
  mbi.long <- data %>% pivot_longer(everything(), names_to = "item", values_to = "score")

mbi.long %>% 
  ggplot(aes(x = item, y = score)) +
  geom_violin(trim=F, fill = "#9999CC") +
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 45, hjust =1),
        axis.title = element_text(size = 12),
        panel.background = element_rect(fill = "white", color = "black"),
        plot.title = element_text(face="bold",
                                  hjust = 0.5),
        axis.title.x = element_blank())+
  labs(x = "Item", 
       y = "Score",
       title = paste("Figure 2 ", fig.num, " Violin plot of the selected items"))+
  geom_boxplot(width = 0.1, fill = "white")
}
```

### To generate a function describing continuous data set

```{r, echo=F}
descriptive <- function(data){
  library(finalfit)
library(kableExtra)
inspect.table <- ff_glimpse(data)$Continuous
inspect.table$label <- NULL
inspect.table %>% 
  mutate('Q1Q3' = paste(quartile_25, 
                        quartile_75, 
                        sep = " ~ ")) %>% 
  select(n, 
         'n of NA' = missing_n, 
         'Mean' = mean, 
         'Median' = median,
         'SD' = sd, 
         'Min' = min, 
         'Max' = max,
         'Q1~Q3' = Q1Q3) %>%
  kable(booktabs = T,  
        align = "r",
        longtable = T,
        linesep = "",
        caption = "Descriptive statistics for measurements") %>% 
  add_header_above(c(" ", 
                     " " = 2,
                     "Central tendency" = 2, 
                     "Dispersion tendency" = 4)) %>% 
  kable_styling(latex_options = c("striped", 
                                  "repeat_header")) %>% 
  column_spec(1, width = "3cm")
}
```

### Write a function describing continuous data set

```{r, echo=F}
descriptive <- function(data){
  library(finalfit)
library(kableExtra)
inspect.table <- ff_glimpse(data)$Continuous
inspect.table$label <- NULL
inspect.table %>%
  mutate('Q1Q3' = paste(quartile_25,
                        quartile_75,
                        sep = " ~ ")) %>%
  select(n,
         'n of NA' = missing_n,
         'Mean' = mean,
         'Median' = median,
         'SD' = sd,
         'Min' = min,
         'Max' = max,
         'Q1~Q3' = Q1Q3) %>%
  kable(booktabs = T,
        align = "r",
        longtable = T,
        linesep = "",
        caption = "Descriptive statistics for measurements") %>%
  add_header_above(c(" ",
                     " " = 2,
                     "Central tendency" = 2,
                     "Dispersion tendency" = 4)) %>%
  kable_styling(latex_options = c("striped",
                                  "repeat_header")) %>%
  column_spec(1, width = "3cm")
}
```

### Write a function for histogram overlapping with density plot

```{r, echo = F}
corr.density <- function(data, fig.num = 1, group){
  data %>%
  pivot_longer(everything()) %>%  #longer format
  ggplot(aes(x = value)) + #x axis used variable "value" (a default of pivot)
  geom_histogram(binwidth = 1, aes(y = ..density..), #match ys of density and histogram plots
                 color = "black",  fill = "#9999CC")+  # adjust aesthetics for hist
  geom_density(fill = "pink", alpha = 0.25)+ #adjust aesthetics for density plot
  facet_wrap(~name, scales = "free", ncol =5) + #wrap by name variable
  theme(panel.grid.major = element_blank(), #get rid of the  grids
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "white",#adjust the background
                                        color = "black"),
        strip.background = element_rect(color = "black",#adjust the strips aes
                                        fill = "steelblue"),
        strip.text = element_text(size =8, color = "white"), #adjust strip text
        axis.title.x = element_text(size = 3), #adjust the x text
        axis.title.y = element_text(size = 3), # adjust the y text
        plot.title = element_text(size = 12,
                                  face = "bold",
                                  hjust = 0.5))+ #adjust the title
  labs(title = paste("Figure", fig.num," Distribution of selected items for", group)) #title it
  }
```

### Write a function to generate dot distribution plot

```{r, echo=F}
dot.dist <- 
  function(data, type, title){
    data |>
      t() |> 
      as.data.frame() %>% 
      mutate(Item = rownames(.)) |> 
      rowwise() |> 
      mutate(Median = eval(parse(text = type))(V1:V580)) |> 
      ggstatsplot::ggdotplotstats(
        point.args = list(color = "red", size = 3, shape = 13),
        xlab = paste(type, "ratings"),
        title = title,
        x = Median,
        y = Item
      )
    }
```

### Write a fuction to generate correlation matrix with statistical test

```{r, echo=F}
mycor <- 
  function(data, cols, title){
  mbi.elm |> 
      select(all_of(cols)) |> 
      ggstatsplot::ggcorrmat(
        colors = c("#B2182B", "white", "#4D4D4D"),
        title = "(a) Items on emotional exhaustion, 
        elementary school teacher",
        matrix.type  = "lower"
      )
    }
```

# Inspect the data

## Distribution of values

```{r,fig.width= 8, fig.height = 13}
#generate the plots, by subgroup of teachers
p.dist.elm <- 
  corr.density(
    mbi.elm, 
    fig.num = "1(a)", 
    group = "elementary school teacher"
    )
p.dist.sec <- 
  corr.density(
    mbi.sec, 
    fig.num = "1(b)",
    group = "secondary school teacher"
    )
#print the plot
library(patchwork); p.dist.elm/p.dist.sec
```

## Distributions of Item statistics (median)

```{r,fig.width= 8, fig.height = 5}
#generate plot by subgroups of teachers
p.dot.elm <- 
  dot.dist(
    data = mbi.elm, type = "median", 
    title = "(a) Elementary school teacher"
    )
p.dot.sec <- 
  dot.dist(
    data = mbi.sec,  type = "median", 
    title = "(b) Secondary school teacher"
    )
#plot layout
patchwork <- p.dot.elm|p.dot.sec
#print the plot with a general title
patchwork+plot_annotation(
    title = 
      'Figure 2 Distributions of median rating for each item',
    theme = 
      theme(plot.title = 
              element_text(
                size = 16,
                face = "bold",
                vjust = -1.5,
                hjust =0.5)
            )
    )
```

## Correlation

```{r,fig.width= 10, fig.height = 15}
fa.ee <- c("ITEM1", "ITEM3", "ITEM6", "ITEM8", "ITEM13", "ITEM14", "ITEM16", "ITEM20")
fa.dp <- c("ITEM5", "ITEM10", "ITEM11", "ITEM15", "ITEM22")
fa.pa <- c("ITEM4", "ITEM7", "ITEM9", "ITEM12", "ITEM17", "ITEM18", "ITEM19", "ITEM21")
#generate 6 plots, 3 factors X 2 subgroups of teachers
p.cor.elm.ee <- 
       mycor(
         data= mbi.elm, cols = fa.ee, 
         "(a) Items on emotional exhaustion, 
         elementary school teacher"
         )
p.cor.sec.ee <- 
       mycor(
         data = mbi.sec,  cols = fa.ee, 
         "(b) Items on emotional exhaustion, 
          secondary school teacher"
         )
p.cor.elm.dp <- 
       mycor(
         data = mbi.elm, cols = fa.dp, 
         "(c) Items on depersonalization,
          elementary school teacher"
         )
p.cor.sec.dp <- 
       mycor(
         data = mbi.sec, cols = fa.dp, 
         "(d) Items on depersonalization,
          secondary school teacher"
         )
p.cor.elm.pa <- 
       mycor(
         data = mbi.elm, cols = fa.pa, 
         "(e) Items on personal accomplishment,
         secondary school teacher"
         )
p.cor.sec.pa <- 
       mycor(
         data = mbi.sec, cols = fa.pa, 
         "(f) Items on personal accomplishment,
          secondary school teacher"
         )
#plot sub-figure layout
patchwork <- 
  p.cor.elm.ee/p.cor.elm.dp/p.cor.elm.pa|p.cor.sec.ee/p.cor.sec.dp/p.cor.sec.pa 
#print the plot with a gernal title
patchwork+
  plot_annotation(
    title = 
      'Figure 3 Correlalogram for items on each factor for two groups of teachers',
    theme = 
      theme(plot.title = 
              element_text(size = 16, face = "bold", vjust = -1.5, hjust =0.5)))
```

# Establishing the baseline model for testing factorial equivalence

Here is a quick summary of how I understand the section (Not sure if it is correct): To test factorial in-variance (equivalence), we need a baseline model that fits across the sub-groups, and that does not have any equality constraints. This model represents the best fitting one balanced with parsimony for EACH group. With this baseline models (one for each group) at hand, I could merge them to obtain a configural model, which represents the best fitting model balanced with parsimony for both group simultaneously (also importantly, without any between-group in-variance assumed). This configural model is an important pivot with which every model with some equality constraints on factor loadings should be compared in the following steps.

## Define and estimate initial models (for both subgroups)

\textcolor{blue}{The postulated three-factor structure of the MBI that was tested in the previous assignments were re-tested as the initial model for establishing a baseline model. }

### Define the initial model

```{r}
library(lavaan)
# Define a CFA model using the lavaan (Latent Variable Analysis) syntax:
# see https://lavaan.ugent.be/tutorial/syntax1.html
initial.model <- '
# CFA model for the burnout, the baseline model:
    EE =~ ITEM1 + ITEM2 + ITEM3 + ITEM6 + ITEM8 + 
          ITEM13 + ITEM14 + ITEM16 + ITEM20
    DP =~ ITEM5 + ITEM10 + ITEM11 + ITEM15 +ITEM22
    PA =~ ITEM4 + ITEM7 + ITEM9 + ITEM12 + 
          ITEM17 + ITEM18 + ITEM19 + ITEM21
          '
```

Cited from Byrne: *It is important to note that measuring instruments are often group specific in the way they operate, and, thus, it is possible that baseline models may not be completely identical across groups.*

### Estimate indices to examine factorial validity

(1) Estimate factorial validity for the elementary teacher subgroup

```{r}
cfa.elm <- 
  cfa(
    initial.model, 
    data = mbi.elm,  
    estimator = "MLM",
    mimic = "Mplus"
    )
```

(2) Estimate factorial validity for the secondary teacher subgroup

```{r}
cfa.sec <- 
  cfa(
    initial.model, 
    data = mbi.sec,  
    estimator = "MLM",
    mimic = "Mplus"
    )
```

### Evaluate model

(1) Fit indices

```{r}
library(knitr);library(kableExtra)
#combine fit indices of both levels
initial.elm.fit <- 
  cfa.summary.mlm.a(cfa.elm) |> 
  t() |> 
  as.data.frame()

initial.sec.fit <- 
  cfa.summary.mlm.a(cfa.sec) |> 
  t() |> 
  as.data.frame()

initial.both <- 
  rbind(
    initial.elm.fit[2,], 
    initial.sec.fit[2,]
    ) 

names(initial.both) <- 
  initial.elm.fit[1,]

rownames(initial.both) <- NULL

initial.both <- 
  initial.both |> 
  mutate(Model = c("Elementary level",
    "Secondary level")) |> 
  select(Model, everything())

#print the table
multi.fit.tab(initial.both, "Fit indices for two subgroups(initial model)")
```

\textcolor{blue}{See table 1. Goodness-of-fit statistics for this baseline model (three factor) reveals that the indices are less than optimal} for both elementary (MLM Chi-square[206] = 826.573; CFI = 0.857; RMSEA = 0.072 ; SRMR = 0.068) and secondary (MLM Chi-square[206] = 999.359; CFI = 0.836; RMSEA = 0.075; SRMR = 0.077) levels. \textcolor{blue}{Model re-specification should be the next step.}

(2) factor loading

Factor loading of elementary level were extracted.

```{r}
fl.elm <- cfa.summary.b (cfa.elm) #fl  is for factor loading)
colnames(fl.elm)[2] <- "Beta*"
```

Factor loading of secondary level were extracted.

```{r}
fl.sec <- cfa.summary.b (cfa.sec) #fl is for factor loading
colnames(fl.sec) <- c("Parameter",
                     "Beta* ",
                     "SE ",
                     "Z ",
                     "p-value ")
```

Factor loading of both levels were merged in one table and printed.

```{r}
fl.both <- left_join(fl.elm, 
                     fl.sec, 
                     by = "Parameter")
fl.both |> 
  kable(
    digits = 3,
    booktabs = T,
    #format = "markdown",
    caption = "Factor loadings for both levels (initial model)",
    linesep = ""
    ) |>  
  add_header_above(c(" " = 1, 
                     "Elementary level" = 4,
                     "Secondary level" = 4
                     )
                   ) |> 
  kable_styling() |> 
  row_spec(1:9, 
           background = "#E5E4E2"
           ) |> 
  row_spec(15:22, 
           background = "#E5E4E2"
           ) |> 
  row_spec(c(1,10,15), bold = T) |> 
  footnote(general = 
             "Rows with coeffcient estimates fixed to 1 are highligted in bold ",
           symbol = c(
             "Standardized estimates"
                      )
           )
  
```

the cross-loading involved the loading of Item 12 on Factor 1 (Emotional Exhaustion) in addition to its targeted Factor 3 (Personal Accomplishment)

(3) Variance

Variance of elementary level were extracted.

```{r}
var.elm <- cfa.summary.c(cfa.elm, fa.num = 3, item.num = 22)
names(var.elm)[3] <- "Beta*"
names(var.elm)[4]<- "Beta†"
```

Variance of secondary level were extracted.

```{r}
var.sec <- cfa.summary.c(cfa.sec, fa.num = 3, item.num = 22)
var.sec <- var.sec[,-1]
names(var.sec) <- 
  c("Indicator", 
    "Beta* ", 
    "Beta† ",
    "SE ", 
    "Z ", 
    "p-value "
    )
```

Variance of both levels were merged in one table and printed.

```{r}
var.both <- left_join(var.elm, 
                     var.sec, 
                     by = "Indicator")

align.table(data = var.both, 
            num.no.header.col = 2, 
            title = "Residual variance for both levels (initial model)")
```

(3) Co-variance

Co-variance of elementary level were extracted.

```{r}
cov.elm <- cfa.summary.d(cfa.elm, fa.num = 3, item.num = 22)
colnames(cov.elm)[2:3] <- c("Beta*", "Beta†")
```

Co-variance of secondary level were extracted.

```{r}
cov.sec <- cfa.summary.d(cfa.sec, fa.num = 3, item.num = 22)
colnames(cov.sec) <- c("Parameter", "Beta* ", "Beta† ", "SE ", "Z ", "p-value ")
```

Co-variance of both levels were merged in one table and printed.

```{r}
cov.both <- left_join(cov.elm, 
                     cov.sec, 
                     by = "Parameter")

align.table(data = cov.both, 
            num.no.header.col = 1, 
            title = "Residual co-variance for both levels (initial model)")
```

### Model re-specification

(1) Search for mis-specified parameters

Last step, although I didn't find parameters of strongly aberrant estimates,un-satisfactory fit indices still indicated model misfit. \textcolor{blue}{ Hence, in the current section I performed model re-specification in searching for better-fitting models. } Specifically, to establish baseline models for both panels of teachers that represent good model fit and parsimony, I further investigated the modification indices of the hypothesized models, respectively for two levels.

MIs of elementary level panel were calculated.

```{r}
#extract needed variables
initial.MI.elm <- 
  modindices(cfa.elm,
             standardized = TRUE,
             sort. = TRUE,
             maximum.number = 10)
```

MIs of secondary level panel were calculated.

```{r}
#extract needed variables
initial.MI.sec <- 
  modindices(cfa.sec,
             standardized = TRUE,
             sort. = TRUE,
             maximum.number = 10)
```

MI tables with 10 largest MI parameters was printed in descending order of MI. Potential mis-specification of most concerns were highlighted in red.

```{r}
MI.both <- rbind(initial.MI.elm, initial.MI.sec)

MI.both    |> 
  mutate(
    op = case_when(op == "~~"~"←→",
                   op == "=~"~"→"), 
    Parameter = 
           paste(lhs, op, rhs)
         ) |>
  select(Parameter, 
         MI = mi, 
         EPC = epc, 
         "std EPC" = sepc.all
         )|>
  kable(digits = 3,
        booktab = T,
        linesep = "",
        caption = 
          "Selected modification indices (initial model)") |>
  kable_styling(
    latex_options = "striped"
    ) |>
  row_spec(
    c(1:4, 11:14), 
    color = "red"
    ) |> 
  footnote(general = 
             "Rows highlighted in red are of special concerns") |> 
  pack_rows(index = c(
    "Elementary level" = 10,
    "Secondary level" = 10
    )
    )
```

\textcolor{blue}{See table 5. Three exceptionally large residual co-variances and one cross-loading contributed to the misfit of the model for both teacher panels.} The residual co-variances involved Items 1 and 2, Items 6 and 16, and Items 10 and 11; the cross-loading involved the loading of Item 12 on Factor 1 (Emotional Exhaustion) in addition to its targeted Factor 3 (Personal Accomplishment).

In reviewing both the MIs and expected parameter change (EPC) statistics for elementary teachers (table 5, upper part), it is clear that all four parameters are contributing substantially to model misfit, with the residual covariance between Item 6 and Item 16 exhibiting the most profound effect.

We see precisely the same pattern on secondary teachers, albeit the effect would appear to be even more pronounced than it was for elementary teachers. One slight difference between the two groups of teachers regards the impact of these four parameters on model misfit. Whereas the residual covariance between Items 6 and 16 was found to be the most seriously misfitting parameter for elementary teachers; for secondary teachers, the residual covariance between Items 1 and 2 was most pronounced.

(2) Re-specify initial model to model 2

The good practice is relaxing one parameter each time. \textcolor{blue}{Nonetheless, according to the knowledge derived from our previous work, I included all four mis-specified parameters in a post-hoc model (common to the groups).}

First, the 4 parameters were relaxed in model statement.

```{r}
respecified4 <- 'EE =~ ITEM12
                 ITEM6 ~~ ITEM16
                 ITEM10 ~~ ITEM11
                 ITEM1 ~~ ITEM2
                 '
model2 <- paste(initial.model, respecified4)
```

Then, the model fit were re-estimated for both group, respectively

```{r}
#for elementary
cfa2.elm <- 
  cfa(
    model2, 
    data = mbi.elm,  
    estimator = "MLM",
    mimic = "Mplus"
    )
#for secondary
cfa2.sec <- 
  cfa(
    model2, 
    data = mbi.sec,  
    estimator = "MLM",
    mimic = "Mplus"
    )
```

## Establish Model 2 (for both groups of teachers)

(1) Inspect fit indices of model2 (comparing to initial model)

```{r}
#combine fit indices of both levels
model2.elm.fit <- 
  cfa.summary.mlm.a(
    cfa2.elm
    ) |> 
  t() |> 
  as.data.frame()

model2.sec.fit <- 
  cfa.summary.mlm.a(
    cfa2.sec
    ) |> 
  t() |> 
  as.data.frame()

model2.both <- 
  rbind(
    model2.elm.fit[2,], 
    model2.sec.fit[2,]
    ) 

names(model2.both) <- model2.elm.fit[1,]

rownames(model2.both) <- NULL

model2.both <- 
  model2.both |> 
  mutate(Model = c("Elementary level",
    "Secondary level")) |> 
  select(Model, everything())

#combine model 1 and 2 tables
compare12 <- rbind(initial.both, model2.both)

#print the table
multi.fit.tab(compare12, 
              "Fit indices for two subgroups, model 2, comparing to initial model") |> 
  pack_rows(index = c(
    "Initial model" = 2,
    "Model 2" = 2
  )
  )
```

\textcolor{blue}{Estimation of this re-specified model, for each teacher group, yielded greatly improved model fit statistics than initial model. See table 6. However, we should note that several statistics, albeit improved comparing to initial model, still fall below the preferable value. For example, CFIs and TLIs were <0.95.}

(2) Modification indices of model 2

To establish baseline models for both panels of teachers that represent good model fit and parsimony, \textcolor{blue}{I further investigated the modification indices of model 2, respectively for two groups, to decide if there was any more model mis-fit and mis-specification }

MIs of elementary level panel were calculated.

```{r}
model2.MI.elm <- 
  modindices(cfa2.elm,
             standardized = TRUE,
             sort. = TRUE,
             maximum.number = 10)
```

MIs of secondary level panel were calculated.

```{r}
model2.MI.sec <- 
  modindices(cfa2.sec,
             standardized = TRUE,
             sort. = TRUE,
             maximum.number = 10)
```

MI tables with 10 largest MI parameters was printed in descending order of MI. Potential mis-specification of most concerns were highlighted in red.

```{r}
MI2.both <- rbind(model2.MI.elm, model2.MI.sec)

MI2.both    |> 
  mutate(
    op = case_when(op == "~~"~"←→",
                   op == "=~"~"→"), 
    Parameter = 
           paste(lhs, op, rhs)
         ) |>
  select(Parameter, 
         MI = mi, 
         EPC = epc, 
         "std EPC" = sepc.all
         )|>
  kable(digits = 3,
        booktab = T,
        linesep = "",
        caption = 
          "Selected modification indices (model 2)") |>
  kable_styling(
    latex_options = "striped"
    ) |>
  row_spec(
    c(1:2, 11:12), 
    color = "red"
    ) |> 
  footnote(general = 
             "Rows highlighted in red are of special concerns") |> 
  pack_rows(index = c(
    "Elementary level" = 10,
    "Secondary level" = 10
    )
    )
```

\textcolor{blue}{See table 7. In reviewing this information for elementary teachers, we observe two MIs larger than all other MIs (ITEM7 with ITEM4; ITEM19 with ITEM18); both represent residual co-variances. I followed Byrne's step in addressing the parameter (set the residusal co-variance between ITEMs 7 and 4 free to estimate for elementary teachers).} Of the two, only the residual covariance between Items 7 and 4 is substantively viable in that there is a clear overlapping of item content. In contrast, the content of Items 19 and 18 exhibits no such redundancy, and, thus, there is no justification for including it in a succeeding Model 3.

\textcolor{blue}{In checking the MI for secondary teachers, again I decided to re-specifiy the model in establishing an appropriate baseline model.} Two parameters were of concern due to large MI and substantive meaningfulness. They are Item 11 cross-loads onto factor EE, and item 19 co-varies with item 9. This time I operated by the good practice of specifying one parameter each time. Given the substantially large MI representing the cross-loading of Item 11 on factor EE, this parameter alone was included in our next post-hoc model (Model 3 for secondary teachers).

Byrne noted the reasons for making this decision (to further re-specifying model secondary teachers), which I quoted here for future reflection: (a) The model does not yet reflect a satisfactorily good fit to the data (CFI = 0.920); and (b) in reviewing the MIs in Table 7.2, we observe one very large mis-specified parameter representing the loading of Item 11 on Factor 1 (F1 by ITEM11), as well as another substantially large MI representing a residual covariance between Items 19 and 9, both of which can be substantiated as substantively meaningful parameters.

(3) Model re-specification of model 2 to model 3

```{r}
respecified3.elm <- 'ITEM4 ~~ ITEM7
                     '
respecified3.sec <- 'EE =~ ITEM11
                     '
model3.elm <- paste(model2, respecified3.elm)                 
model3.sec <- paste(model2, respecified3.sec)
```

Then, the model fit were re-estimated for both group, separately.

```{r}
#for elementary
cfa3.elm <- 
  cfa(
    model3.elm, 
    data = mbi.elm,  
    estimator = "MLM",
    mimic = "Mplus"
    )
#for secondary
cfa3.sec <- 
  cfa(
    model3.sec, 
    data = mbi.sec,  
    estimator = "MLM",
    mimic = "Mplus"
    )
```

## Establish Model 3 (for both groups of teachers)

(1) Inspect fit indices of model3 (comparing to model 2)

```{r}
#combine fit indices of both levels
model3.elm.fit <- 
  cfa.summary.mlm.a(
    cfa3.elm
    ) |> 
  t() |> 
  as.data.frame()

model3.sec.fit <- 
  cfa.summary.mlm.a(
    cfa3.sec
    ) |> 
  t() |> 
  as.data.frame()

model3.both <- 
  rbind(
    model3.elm.fit[2,], 
    model3.sec.fit[2,]
    ) 

names(model3.both) <- model3.elm.fit[1,]
rownames(model3.both) <- NULL

model3.both <- 
  model3.both |> 
  mutate(Model = c("Elementary level",
    "Secondary level")) |> 
  select(Model, everything())

#combine model 1 and 2 tables
compare123 <- rbind(initial.both, model2.both, model3.both)

#print the table
multi.fit.tab(compare123, 
              "Fit indices for two subgroups, model 3, comparing to preceding models") |> 
  pack_rows(index = c(
    "Initial model" = 2,
    "Model 2" = 2,
    "Model 3" =2
  )
  )
```

See table 8. Results from the estimation of Model 3 for elementary teachers yielded goodness-of-fit statistics that represented a satisfactorily good fit to the data (MLM chi square[201] = 451.061; CFI = 0.942; RMSEA = 0.046; SRMR = 0.049). Although a review of Table 9 (find below) reveals several additional moderately large MIs, for balancing goodness-of-fit and parsimony, \textcolor{blue}{the decision was model 3 can serve as the baseline model for elementary teachers.}

Results from the estimation of Model 3 for secondary teachers, on the other hand, further substantiated the residual covariance between Items 19 and 9 as representing an acutely mis-specified parameter in the model. \textcolor{blue}{Thus, for secondary teachers only, model 4 was put to the test with this residual covariance specified as a freely estimated parameter.}

(2) Modification indices of model 3

MIs of model 3 for each groups were calculated.

```{r}
#elementary
model3.MI.elm <- 
  modindices(cfa3.elm,
             standardized = TRUE,
             sort. = TRUE,
             maximum.number = 10)
#secondary
model3.MI.sec <- 
  modindices(cfa3.sec,
             standardized = TRUE,
             sort. = TRUE,
             maximum.number = 10)
```

MI tables with 10 largest MI parameters was printed in descending order of MI. Potential mis-specification of most concerns were highlighted in red.

```{r}
MI3.both <- rbind(model3.MI.elm, model3.MI.sec)

MI3.both    |> 
  mutate(
    op = case_when(op == "~~"~"←→",
                   op == "=~"~"→"), 
    Parameter = 
           paste(lhs, op, rhs)
         ) |>
  select(Parameter, 
         MI = mi, 
         EPC = epc, 
         "std EPC" = sepc.all
         )|>
  kable(digits = 3,
        booktab = T,
        linesep = "",
        caption = 
          "Selected modification indices (model 3)") |>
  kable_styling(
    latex_options = "striped"
    ) |>
  row_spec(
    c(1:2, 11), 
    color = "red"
    ) |> 
  footnote(general = 
             "Rows highlighted in red are of special concerns") |> 
  pack_rows(index = c(
    "Elementary level" = 10,
    "Secondary level" = 10
    )
    )
```

(3) Re-specification of model 3 to model 4 (only for secondary teacher)

The parameter ITEM9 \~\~ ITEM19 was relaxed for estimation.

```{r}
respecified4.sec <- 'ITEM9 ~~ ITEM19
                 '
model4.sec <- paste(model3.sec, respecified4.sec)
```

Then, the model fit were re-estimated for secondary group, only

```{r}
cfa4.sec <- 
  cfa(
    model4.sec, 
    data = mbi.sec,  
    estimator = "MLM",
    mimic = "Mplus"
    )
```

## Establish Model 4 (for secondary tearchers only)

Note that at this point I had already taken model 3 as the baseline model for elementary teachers, and \textcolor{blue}{model 4 was to achieve the baseline model for secondary teachers.}

(1) Inspect fit indices of model4 (comparing to 3)

```{r}
model4.sec.fit <- 
  cfa.summary.mlm.a(
    cfa4.sec
    ) |> 
  t() |> 
  as.data.frame()
names(model4.sec.fit ) <- model4.sec.fit[1,]
model4.sec.fit <- model4.sec.fit [-1,]
model4.sec.fit  <- 
  model4.sec.fit  |> 
  mutate(Model = "Secondary level") |> 
  select(Model, everything())
rownames(model4.sec.fit ) <- NULL
#combine model 1 and 2 tables
model3.both[1,1] <- "Elementary level†"
model4.sec.fit[1,1] <- "Secondary level‡"
compare1234 <- 
  rbind(initial.both,
        model2.both, 
        model3.both, 
        model4.sec.fit 
        )
#print the table
key.table1 <- multi.fit.tab(compare1234, 
              "Fit indices for two subgroups, model 4, comparing to preceding models",
              c("Baseline model for elementary teachers", 
                "Baseline model for secondary teachers")) |> 
  pack_rows(index = c(
    "Initial model" = 2,
    "Model 2" = 2,
    "Model 3" =2,
    "Model 4" =1 )
  ) |> 
  row_spec(c(5,7), 
           color = "red"
           )
key.table1
```

See table 10. Based on a moderately satisfactory goodness-of-fit (MLM cgi-square[200] = 505.831; CFI = 0.937; RMSEA = 0.047; SRMR = 0.052) and to balance fit with parsimony, \textcolor{blue}{I consider Model 4 as the final baseline model for secondary teachers. }

```{r}
cfa3.elm <- 
  cfa(
    model3.elm, 
    data = mbi.elm,  
    estimator = "MLM"
    )
cfa4.sec <- 
  cfa(
    model4.sec, 
    data = mbi.elm,  
    estimator = "MLM"
    )
```

## Visualize the final baseline models for each group

```{r, fig.width= 9, fig.height= 12, fig.align='center'}
library(semPlot)
grps <- list(EE = c("ITEM1", "ITEM2", "ITEM3", "ITEM6", "ITEM8", 
        "ITEM13", "ITEM14",  "ITEM16", "ITEM20"), 
        DP = c("ITEM5", "ITEM10", "ITEM11", "ITEM15","ITEM22"),
        PA = c("ITEM4", "ITEM7", "ITEM9", "ITEM12",
               "ITEM17", "ITEM18", "ITEM19", "ITEM21"))
order.manifest <- c("ITEM4", "ITEM7", "ITEM9", "ITEM12",
        "ITEM17", "ITEM18", "ITEM19", "ITEM21",
        "ITEM5", "ITEM10", "ITEM11", "ITEM15","ITEM22",
        "ITEM1", "ITEM2", "ITEM3", "ITEM6", "ITEM8", 
        "ITEM13", "ITEM14",  "ITEM16", "ITEM20")
order.latent <- c("PA", "DP", "EE")
par(mfrow=c(1,2))
semPaths(cfa3.elm, 
         "col", #un-weighted edges
         "no", #edge label is standarized
         reorder = F,
         latents = order.latent,
         manifest = order.manifest,
         sizeLat = 8,
         sizeLat2 = 5,
         sizeMan = 6,
         sizeMan2 = 3,
         curveAdjacent = "cov", # if edge for adjacent nodes curly or not, "reg"
         shapeMan = "rectangle",
         style = "lisrel",
         group = "latent",
         curve = 0.3,
         curvature = 0.1,#theme = "colorblind",#cardinal = "lat cov",
         curvePivot = F,# curly edge or not
         rotation = 2,
         color = c("#c68642", "#58668b", "#8874a3"),#edge.color = "steelblue",
         shapeLat = "ellipse",
         label.font = 2,
         label.color = "white", #Label.scale =T,
         label.prop = 0.7
         )
title(main = list("Elementary School Teachers",
                  cex = 1, font =1),outer = F, line = -3)
semPaths(cfa4.sec, 
         "col", #un-weighted edges
         "no", #edge label is standarized
         reorder = F,
         latents = order.latent,
         manifest = order.manifest,
         sizeLat = 8,
         sizeLat2 = 5,
         sizeMan = 6,
         sizeMan2 = 3,
         curveAdjacent = "cov", #if edge for adjacent nodes curly or not, "reg"
         shapeMan = "rectangle",
         style = "lisrel",
         group = "latent",
         curve = 0.3,
         curvature = 0.1,#theme = "colorblind", #cardinal = "lat cov",
         curvePivot = F, # curly edge or not
         layout = "tree",
         rotation = 2,
         color = c("#c68642", "#58668b", "#8874a3"), #edge.color = "steelblue",
         shapeLat = "ellipse",
         label.font = 2,
         label.color = "white",#Label.scale =T,
         label.prop = 0.7
         )
title(main = list("Secondary School Teachers",
                  cex = 1, font =1), outer = F, line = -3)
mtext("Figure 4 Baseline MBI models for two groups of teacher",
      cex = 1.5, side = 1, line = -5, outer = TRUE)
```

\textcolor{blue}{The the plot of baseline model for each group of teachers was created. See figure 4.} There are three parameters (two residual co-variances [Item 4 with Item 7; Item 9 with Item 19] and one cross-loading [Item 11 on F1]) that were not part of the originally postulated model and that differ across the two groups of teachers. (\textcolor{red} { According to slides, they (the diagrams) have the same number of factors and the same factor-loading pattern. A question: Is there any standard for such a pattern? The current baseline models have one cross-loading that differs across the groups so they are not exactaly same.})

## Section Summary

In this section I started by defining an initial model for each group of teacher (nested model). Then I fine-tuned the model into several less and less restrictive models. Evolving into model3, I found the best fitting balanced with parsimony model for elementary teachers. One more model later, I also decided upon the one for secondary teachers. They served as the baseline models, from which the configural model were generated in next section.

# Testing factorial equivalence of MBI between elementary and secondary shcool teachers

\textcolor{blue}{With the baseline models established through the previous steps, I could combine them to generate a common baseline model for both group, or configural model}, which is a pivotal step for testing factorial in-variance across groups.

## Establish configural model (inv1.model)

### Combine the datasets

```{r}
mbi.both <- 
  merge(
    data.frame(
      mbi.elm, 
      group = "elementary"
      ),
    data.frame(
      mbi.sec, 
      group = "secondary"
      ),
    all = TRUE, 
    sort = FALSE
    )
```

### Define the configural model

No equality constraints are imposed for this model.

```{r}
inv1.model <- '
    EE =~ 1*ITEM1 + ITEM2 + ITEM3 + ITEM6 + ITEM8 + ITEM13 + ITEM14 + ITEM16 + ITEM20
    DP =~ 1*ITEM5 + ITEM10 + ITEM11 + ITEM15 + ITEM22
    PA =~ 1*ITEM4 + ITEM7 + ITEM9 + ITEM12 + ITEM17 + ITEM18 + ITEM19 + ITEM21

# Common modifications (from baseline models built above)
    EE =~ ITEM12 # common cross-loading
 ITEM1 ~~ ITEM2  # common residual covariances (3)
 ITEM6 ~~ ITEM16
ITEM10 ~~ ITEM11

# Group-specific parameters for elementary teachers:
 ITEM4 ~~ c(NA, 0)*ITEM7  # specific residual covariance

# Group-specific parameters for secondary teachers:
    EE =~ c(0, NA)*ITEM11 # specific cross-loading
 ITEM9 ~~ c(0, NA)*ITEM19 # specific residual covariance
'
```

### Estimate the configural model

The model fit results derived from this model represent a multi-group version of the combined baseline models for elementary and secondary teacher.

```{r}
inv1.fit <- 
  cfa(
    inv1.model, 
    data = mbi.both, 
    estimator = "MLM", 
    group = "group"
    )
```

### Summarize the results

```{r}
#extract the key indicators
inv1.fit.indices <- 
  cfa.summary.mlm.a(inv1.fit) |> 
  t() |> 
  as.data.frame()

#define column and row names for the indicator table
names(inv1.fit.indices) <- inv1.fit.indices[1,]
inv1.fit.indices <- inv1.fit.indices[-1,]
rownames(inv1.fit.indices) <- NULL
inv1.fit.indices$Model <-"Configural (inv1)"

#print the table
multi.fit.tab(
  inv1.fit.indices, 
  "Fit indices for the configural model (inv1.model)"
  )
```

See table 11. Results for this configural model (inv1.model) were as follows: MLM chi-square(401) = 939.696, CFI = 0.939, RMSEA = 0.046, and SRMR = 0.051.

## Impose and relax equality constraints on factor loadings of configural model

### Constrain all common factor loadings (inv2.model)

(1) Define and inspect the fit indices of model 2 (inv2.model)

\textcolor{blue}{All the common factor loadings were constrained equal across groups.} If the results show significant improvement from configural model, we get the evidence about multi-group in-variance. If not, we need to further explore which parameter(s) bring about the difference observed.

```{r}
inv2.fit <- 
  cfa(inv1.model, 
      data = mbi.both, 
      estimator = "MLM", 
      group = "group",
      group.equal = c("loadings"),
      group.partial = c("EE =~ ITEM11")
  )
```

```{r}
#extract the key indicators
inv2.fit.indices <- 
  cfa.summary.mlm.a(inv2.fit) |> 
  t() |> 
  as.data.frame()

#define column and row names for the indicator table
names(inv2.fit.indices) <- inv2.fit.indices[1,]
inv2.fit.indices <- inv2.fit.indices[-1,]
rownames(inv2.fit.indices) <- NULL
inv2.fit.indices$Model <-"Model2 (inv2)†"

#merge configural model and inv2.model.
fit.indices.12 <- # 12 is for inv1 and inv 2
  rbind(
    inv1.fit.indices, 
    inv2.fit.indices
    ) 

#print the table
multi.fit.tab(
  fit.indices.12, 
  "Comparing Fit indices between the configural model (inv1.model) and model 2 (inv2.model)",
  "Configural model + 20 common factor loadings constrained equal across groups"
  )
```

See table 12. As indicated by the very slightly higher MLM chi-square value (939.696→995.433) and lower CFI value (0.939→0.935), compared with the configural model, results suggest that \textcolor{blue}{the model does not fit the data quite as well as it did with no factor-loading constraints imposed. Thus, we explore furhter to find out the parammeter(s) brought about the non-invariance. }

(2) Examine the modification indices for inv2.model

MIs of inv2.model were calculated. In seeking evidence of non-invariance, we focus only on the factor loadings that were constrained equal across the groups. In addition, in testing for invariance, only those parameters that were constrained equal, are of relevance. Hence, only the parameter statement that meets these requirements were extracted.

First, to simplify the searching for relevant parameters, I defined an object including all parameters were relevant, so that what parameters were shown in MI table were automatically controlled.

```{r}
#create the parameter statements of relevancy
itemset1 <- "ITEM1 + ITEM2 + ITEM3 + ITEM6 + ITEM8 + ITEM13 + ITEM14 + ITEM16 + ITEM20 + ITEM12"
itemset2 <- "ITEM5 + ITEM10 + ITEM11 + ITEM15 + ITEM22"
itemset3 <- "ITEM4 + ITEM7 + ITEM9 + ITEM12 + ITEM17 + ITEM18 + ITEM19 + ITEM21"
#create relevant statement for EE
relevant.items1 <- 
  stringr::str_replace_all(
    stringr::str_split_1(itemset1, "\\+" ), 
    " ", 
    ""
    ) 
relevant.items1 <- 
  paste("EE", "→", relevant.items1)
#create relevant statement for DP
relevant.items2 <- 
  stringr::str_replace_all(
    stringr::str_split_1(itemset2, "\\+" ), 
    " ", 
    ""
    ) 
relevant.items2 <- 
  paste("DP", "→", relevant.items2)
#create relevant statement for PA
relevant.items3 <- 
  stringr::str_replace_all(
    stringr::str_split_1(itemset3, "\\+" ), 
    " ", 
    ""
    ) 
relevant.items3 <- 
  paste("PA", "→", relevant.items3)
#combine the above into one 
relevant.items <- c(relevant.items1, relevant.items2, relevant.items3)
```

Next, I extract MI table with relevant parameters.

```{r}
inv2.model.MI <- modindices(inv2.fit, 
           standardized = TRUE, 
           minimum.value = 3.84, 
           free.remove = FALSE,
           op = "=~", 
           sort. = TRUE) |> 
  mutate(Parameter = paste(lhs, "→", rhs)) |> 
  filter(Parameter %in% relevant.items) |> 
  arrange(group)
```

Then, MI tables with relevant parameters was printed in descending order of MI. Potential parameters that are very possibly undermining equivalence across elementary and secondary teachers were highlighted in red.

```{r}
inv2.model.MI.elm <- 
  inv2.model.MI  |> 
  filter(group == 1) |> 
  select(Parameter, 
         MI = mi, 
         EPC = epc, 
         "std EPC" = sepc.all
         )

inv2.model.MI.sec <- 
  inv2.model.MI  |>  
  filter(group == 2) |> 
  select(Parameter, 
         MI = mi, 
         EPC = epc, 
         "std EPC" = sepc.all
         )


rbind(inv2.model.MI.elm, inv2.model.MI.sec) |> 
  kable(digits = 3,
        booktab = T,
        linesep = "",
        caption = 
          "Selected modification indices for inv2.model") |>
  kable_styling(
    latex_options = "striped"
    ) |>
  pack_rows(index = c("Elementary teachers" = nrow(inv2.model.MI.elm),
                      "Secondary teachers" = nrow(inv2.model.MI.sec)
                      )
            )|>
  row_spec(
    c(1,3), 
    color = "red"
    ) |> 
  footnote(general = 
             "Rows highlighted in red are of special concerns") 
```

\textcolor{blue}{See table 13. Of all the eligible parameters, the factor loading of Item 11 on DP appears to be the most problematic in terms of its group equivalence. In the next step, I relaxed this factor loading(Item 11 by DP) from constraint for establishing the the next model, model3 (inv3.model)}

(3) Re-specify model2 to fit model 3 (inv3.model) by relaxing a parameter

quality constraint on Parameter "DP by ITEM11" was relaxed.

```{r}
inv3.fit <- 
  cfa(inv1.model, 
      data = mbi.both, 
      estimator = "MLM", 
      group = "group",
      group.equal = c("loadings"),
      group.partial = c("EE =~ ITEM11",
                        "DP =~ ITEM11"
                        )
  )
```

### Relax common factor loadings one by one(inv3.model)

Here is a quick summary of how I understand the section (Not sure if it is correct): In the previous sub-section, I imposed equality constraint on all the commonly-estimated factor loading (inv2.model) and the results showed poor fit in comparing to configural model (inv1.model), providing evidence of non-invariance (if they are really equivalent, the statistics should look good when and especially whenwe forced these factor loadings to be equivalent). Then I inspected the MI table and identified a potential source of non-invariance (Parameter DP BY ITEM11). Lastly, I relaxed it and refit the data. In the current sub-section, I continued by inspecting how fit indices and estimates of the re-fit model looks like. The plan was to repeat the parameter relaxing work in a similar way, one parameter a time, until I get a model that is NOT statistically different from the configural model. This done, I could conclude between-group equivalence except for the parameters whose equality constraint I relaxed. In other word, these relaxed parameters are the source of non-invariance, and I find them.

(1) Inspect fit indices of model 3 (inv3.model)

This is to get the fit indices and estimates for the model I fit in the last part of the previous section (inv3.model).

```{r}
#extract the key indicators
inv3.fit.indices <- 
  cfa.summary.mlm.a(inv3.fit) |> 
  t() |> 
  as.data.frame()

#define column and row names for the indicator table
names(inv3.fit.indices) <- inv3.fit.indices[1,]
inv3.fit.indices <- inv3.fit.indices[-1,]
rownames(inv3.fit.indices) <- NULL
inv3.fit.indices$Model <-"Model3 (inv3)‡"

#merge configural model and inv2.model.
fit.indices.123 <- # 123 is for inv1, 2 and 3 models
  rbind(
    inv1.fit.indices, 
    inv2.fit.indices,
    inv3.fit.indices
    ) 

#print the table
delta.fit.tab(fit.indices.123, 
              "Comparing Fit indices of inv3.model with the preceding models",
              c("Configural model + 20 common factor loadings constrained equal across groups",
                "Inv2.model + a parameter(DP By Item11) set relaxed"))
```

```{r}
inv3.fit |> 
  parameterEstimates(standardized=TRUE) |> 
  filter(
    lhs == "DP", 
    op == "=~" ,
    rhs == "ITEM11"
    ) |> 
  mutate(
    group = case_when(group == 1~"Elementary teachers",
                      group == 2~"Secondary teachers"),
    Parameter = paste0(lhs, "→", rhs,"(", group, ")"),
    pvalue = case_when(pvalue >= 0.001 ~ as.character(pvalue),
                  pvalue <0.001 ~ "<0.001")
  ) |> 
  select( "Parameter (level)" = Parameter,
          "Estimates*" = est,
          SE = se,
          "Z-statistics" = z,
          "p-value" = pvalue
  ) |> 
  kable(
    digits = 3,
    booktabs = T,
    caption = "Re-estimates for parameter 'DP =~ ITEM11' (after its equality constraints relaxed)"
  ) |> 
  kable_styling () |> 
  column_spec(1, width = "6.5cm") |> 
  column_spec(2, width = "1.8cm", color = "red") |> 
  column_spec(3, width = "1.2cm") |> 
  column_spec(4, width = "1.8cm") |> 
  column_spec(5, width = "1.2cm") |> 
  footnote(symbol = "Estimates with parameter parameter 'DP =~ ITEM11' set relaxed")
```

\textcolor{blue} {See table 14.} The fit statistics of inv3.model: MLM chi-square[420] 969.990, CFI 0.937, RMSEA 0.045, SRMR 0.054. \textcolor{blue} {The difference in model fit between InvModel 3 and and the configural model (InvModel 1) is, though not significiant, approaching to significance at 0.05 level (p = 0.057).} Moreover, a review of the estimated parameters reveals a fairly substantial difference for the specified parameter (1.095 for elementary and 0.581 for secondary teachers, see table 15). \textcolor{blue} {Hence, I decide to make further improvement to the model.}

\textcolor{red} {In this step, my results were substantially different from the slides. Here in the slides the p value is 0.048, base on which the decision was that we should make further improvement. Yet, I got a insiginificant p for chi-square difference already (0.057, see table 14). I tried several different possibilities. I found if I set "group.partial = c("DP =~ ITEM11")" instead of "group.partial = c("EE =~ ITEM11","DP =~ ITEM11" )" , I would have seen a significant p value very close to the slides. Yet the degree of freedom would turn to 421 instead of 420 (And plus it isn't the correct appraoch, so I didn't really do that). I suppose there should be something I have done wrong. Furthuremore, starting from this step, I saw more big inconsistency with the slides, which I also highlighted in red. I hope these could be solved in next class}

(2) Examine the modification indices for inv3.model

I extract MI table with relevant parameters.

```{r}
inv3.model.MI <- 
  modindices(inv3.fit, 
             standardized = TRUE, 
             minimum.value = 1.00, 
             free.remove = FALSE,
             op = "=~", 
             sort. = TRUE
             ) |> 
  mutate(
    Parameter = 
      paste(lhs, "→", rhs)
    ) |> 
  filter(
    Parameter %in% relevant.items
    ) |> 
  arrange(group)

```

Then, MI tables with relevant parameters was printed in descending order of MI.

```{r}
inv3.model.MI.elm <- 
  inv3.model.MI  |> 
  filter(group == 1) |> 
  select(
    Parameter, 
    MI = mi,
    EPC = epc, 
    "std EPC" = sepc.all
    )

inv3.model.MI.sec <- 
  inv3.model.MI  |>  
  filter(group == 2) |> 
  select(
    Parameter, 
    MI = mi, 
    EPC = epc, 
    "std EPC" = sepc.all
    )


rbind(inv3.model.MI.elm, inv3.model.MI.sec) |> 
  kable(digits = 3,
        booktab = T,
        linesep = "",
        caption = 
          "Selected modification indices for inv3.model") |>
  kable_styling(
    latex_options = "striped"
    ) |>
  pack_rows(index = 
              c("Elementary teachers" = 
                  nrow(inv3.model.MI.elm),
                "Secondary teachers" = 
                  nrow(inv3.model.MI.sec)
                )
            )|>
  row_spec(
    c(1,3,6,8), 
    color = "red"
    ) |> 
  footnote(general = 
             "Rows highlighted in red are of special concerns") 
```

Potential parameters that are very possibly undermining equivalence across elementary and secondary teachers were highlighted in red. \textcolor{blue}{Actually, other than DP measured by ITEM5, which had the largest MI value in both groups of teachers, there are several ohter candidates. }

\textcolor{red} {See table 16. In this step AGAIN, my results were dramatically inconsistent with the slides, I was not able to get the parameter "DP measured by Item15" from MI table until I removed the constraint of minimum MI value of 3.84.  Even this way, the results still pointed me to another tract--parameter "DP by ITEM5" came up in MI table across both groups, and it had MIs larger than the large MI parameter "DP by ITEM15" in slides (Actually, it was not even the second largest on my MI table, see table 16).} I don't think the difference of algorithms between Mplus and lavaan could be so big. I suppose I had done something wrongly which I examined quite hard but failed to identify. Still, I hope I could find out the reason in the next class. Yet, albeit the statistics directed me towards other roads, I still follow the steps of the sildes by relaxing "DP =\~ ITEM15".

(3) Re-specify model2 to fit model 3 (inv3.model)

\textcolor{blue}{Constraint on Parameter "DP =~ ITEM15" was relaxed. }

```{r}
inv4.fit <- 
  cfa(inv1.model, 
      data = mbi.both, 
      estimator = "MLM", 
      group = "group",
      group.equal = c("loadings"),
      group.partial = c("EE =~ ITEM11",
                        "DP =~ ITEM11",
                        "DP =~ ITEM15"
                        ))

```

### Relax common factor loadings one by one(inv4.model)

(1) Inspect fit indices of model 3 (inv4.model)

```{r}
#extract the key indicators
inv4.fit.indices <- 
  cfa.summary.mlm.a(inv4.fit) |> 
  t() |> 
  as.data.frame()

#define column and row names for the indicator table
names(inv4.fit.indices) <- inv4.fit.indices[1,]
inv4.fit.indices <- inv4.fit.indices[-1,]
rownames(inv4.fit.indices) <- NULL
inv4.fit.indices$Model <-"Model4 (inv4)§"

#merge configural model and inv2.model.
fit.indices.1234 <- # 123 is for inv1, 2, 3 and 4 models
  rbind(
    inv1.fit.indices, 
    inv2.fit.indices,
    inv3.fit.indices,
    inv4.fit.indices
    ) 

#print the table
key.table2 <- delta.fit.tab(fit.indices.1234, 
              "Comparison Fit indices of inv4.model with preceding models",
              c("Configural model + 20 common factor loadings constrained equal across groups",
                "Inv2.model + a parameter(DP By Item11) set relaxed",
                "Inv3.model + a parameter(DP By Item15) set relaxed"))
key.table2
```

```{r}
inv4.fit |> 
  parameterEstimates(standardized=TRUE) |> 
  filter(
    lhs == "DP", 
    op == "=~" ,
    rhs == "ITEM15"
    ) |> 
  mutate(
    group = case_when(group == 1~"Elementary teachers",
                      group == 2~"Secondary teachers"),
    Parameter = paste0(lhs, "→", rhs,"(", group, ")"),
    pvalue = case_when(pvalue >= 0.001 ~ as.character(pvalue),
                  pvalue <0.001 ~ "<0.001")
  ) |> 
  select( "Parameter (level)" = Parameter,
          "Estimates*" = est,
          SE = se,
          "Z-statistics" = z,
          "p-value" = pvalue
  ) |> 
  kable(
    digits = 3,
    booktabs = T,
    caption = "Re-estimates for parameter 'DP by ITEM15' (after its equality constraint relaxed)"
  ) |> 
  kable_styling () |> 
  column_spec(1, width = "6.5cm") |> 
  column_spec(2, width = "1.8cm", color = "red") |> 
  column_spec(3, width = "1.2cm") |> 
  column_spec(4, width = "1.8cm") |> 
  column_spec(5, width = "1.2cm") |> 
  footnote(symbol = "Estimates with parameter parameter 'DP by ITEM15' set relaxed")
```

See table 17. Inv4.model produced these results: MLM chi-square[419] 961.653, CFI 0.938, RMSEA 0.045, SRMR 0.054. Comparison of this model with the configural model (see table 17, 3rd column) yielded a corrected MLM chi-square difference of 20.964 (p = 0.281), which is n.s. A review of the estimated parameters revealed a fairly substantial difference for the specified parameter (0.684 for elementary and 0.963 for secondary teachers). See table 18. \textcolor{blue}{Taken together, I decided to stop here and make conclusion that all items on the MBI, except for items 11 and 15, both of which load on Factor2, are operating equivalently across the two groups of teachers.}

The conclusion is--All items on the MBI, except for items 11 and 15, both of which load on Factor2, are operating equivalently across the two groups of teachers.

## Impose equality constraints on residual covariance of configural model

\textcolor{blue}{In this section, I further imposed equality constraints on residual co-variances to test the multi-group in-variance, in addition to the contraints on inv4.model.}

In total, 21 parameters in this model (Inv5.Model) were constrained equal across groups: 17 factor loadings, 1 cross-loading, and 3 residual co-variances.

```{r}
inv5.model <- '
# EE: EmotionalExhaustion
# EP: Depersonalization
# PA: PersonalAccomplishment

    EE =~ 1*ITEM1 + ITEM2 + ITEM3 + ITEM6 + ITEM8 + ITEM13 + ITEM14 + ITEM16 + ITEM20
    DP =~ 1*ITEM5 + ITEM10 + ITEM11 + ITEM15 + ITEM22
    PA =~ 1*ITEM4 + ITEM7 + ITEM9 + ITEM12 + ITEM17 + ITEM18 + ITEM19 + ITEM21

# Common modifications (from baseline models)
    EE =~ ITEM12 # common cross-loading

# Residual covariances (in both groups) - NOW THE FOCUS is on these three:
 ITEM1 ~~ c(a, a)*ITEM2
 ITEM6 ~~ c(b, b)*ITEM16
ITEM10 ~~ c(c, c)*ITEM11

# Group-specific parameters for elementary teachers:
 ITEM4 ~~ c(NA, 0)*ITEM7

# Group-specific parameters for secondary teachers:
    EE =~ c(0, NA)*ITEM11
 ITEM9 ~~ c(0, NA)*ITEM19
'

inv5.fit <- 
  cfa(inv5.model, 
      data = mbi.both,
      estimator = "MLM", 
      group = "group",
      group.equal = c(
        "loadings"
        ),
      group.partial = c(
        "EE =~ ITEM11",
        "DP =~ ITEM11",
        "DP =~ ITEM15"
        )
      ) 
```

```{r}
#extract the key indicators
inv5.fit.indices <- 
  cfa.summary.mlm.a(inv5.fit) |> 
  t() |> 
  as.data.frame()
#define column and row names for the indicator table
names(inv5.fit.indices) <- inv5.fit.indices[1,]
inv5.fit.indices <- inv5.fit.indices[-1,]
rownames(inv5.fit.indices) <- NULL
inv5.fit.indices$Model <-"Model5 (inv5)*"
inv4.fit.indices$Model <-"Model4 (inv4)"
#merge configural model and inv2.model.
fit.indices.45 <- # 45 is for inv4 and 5 models
  rbind(
    inv4.fit.indices,
    inv5.fit.indices
    ) 
#print the table
key.table3 <- delta.fit.tab(fit.indices.45, 
              "Fit indices for the Inv5.model and Inv4.model, for establishing 
              in-variance of residual co-variance",
              c("Inv4.model + 3 residual co-variance constrained euqal across groups"),
              compare = "inv4",
              row.correction = 3)
key.table3 
```

See table 19. Model fit results deviated little from Inv4.model and were as follows: MLM chi-square(422) = 973.378, CFI = 0.937, RMSEA = 0.045, and SRMR = 0.054. Comparison of this model with the previous one (InvModel.4) representing the final model in the test for invariant factor loadings yielded a corrected chi-square difference value of 11.123 (*p* = 0.011), which was statistically significant. (We want it non-significant so that the conclusion that specified residual co-variances between Items 6 and 16, Items 1 and 2, and Items 10 and 11 are operating equivalently across elementary and secondary teachers.)

\textcolor{red}{Yes, Here I got a p value (0.011) indicative of significance, albeit the value was not exactly the same with that on slides (0.014). I suppose this was becasue of the different tools. Besides, "However, we must continue as it was n.s."}

\textcolor{red}{One question here: what is the common procedure for a significant p here? To deal with it, do we do something here? (or go back to factor loading section and make more/less relaxation of parameters),  or do we just get the conclusion that multi-group in-variance do not hold in terms of residual variance?}

The conclusion is--Specified residual co-variances between Items 6 and 16, Items 1 and 2, and Items 10 and 11 are operating equivalently across two groups of teachers.

## Impose equality constraints on structural parameters (i.e., factor variances and covariances)

```{r}
inv6.fit <- 
  cfa(inv5.model, 
      data = mbi.both, 
      estimator = "MLM", 
      group = "group",
      group.equal = c(
        "loadings",
        "lv.variances",
        "lv.covariances"
        ),
      group.partial = c(
        "EE =~ ITEM11",
        "DP =~ ITEM11",
        "DP =~ ITEM15"
        )
              ) 
```

```{r}
inv6.fit.indices <- 
  cfa.summary.mlm.a(inv6.fit) |> 
  t() |> 
  as.data.frame()
#define column and row names for the indicator table
names(inv6.fit.indices) <- inv6.fit.indices[1,]
inv6.fit.indices <- inv6.fit.indices[-1,]
rownames(inv6.fit.indices) <- NULL
inv6.fit.indices$Model <-"Model6 (inv6)*"
inv5.fit.indices$Model <-"Model5 (inv5)"
#merge configural model and inv2.model.
fit.indices.56 <- # 56 is for inv6 and 5 models
  rbind(
    inv5.fit.indices,
    inv6.fit.indices
    ) 
#print the table
key.table4 <-
  delta.fit.tab(
    fit.indices.56, 
    "Fit indices for the Inv6.model and Inv5.model, for establishing in-variance 
    of factor variance/covariance",
    c("Inv5.model + factor variance and co-variance constrained euqal across groups"),
    compare = "inv5",
    row.correction = 4)
key.table4
```

See table 20. The goodness-of-fit statistics for inv6.model were: MLM chi-square[428] 986.389, CFI 0.937, RMSEA 0.045, SRMR 0.059. Comparison with inv5.model yielded a corrected MLM chi-square difference of 12.941 (*p* = 0.044).

\textcolor{red}{Here I got a MLM chi-square difference of 12.941. Though it only deviated slightly from the value on slides (12.117), the small difference made p value fall significant (p = 0.044, vs p= 0.059 on slides). I hope this is due to the difference in algorithms. However, if this were true, we would be surprised by how different tools leads to different conclusions in the areas using SEM.}

The conclusion is--Factor variances and co-variances are operating equivalently across elementary and secondary teachers.

# Summarize the whole invariance testing

To summarize, I categorize the testing for factorial in-variance into the following steps:

   (1) Establish a baseline model for each group. The baseline model can be different across groups, so it might require different number of post-hoc re-specification. The "gold-standard" for a good baseline model is good-fitting and parsimony. \textcolor{red}{See table 21 below, which presents my searching for baseline models from initial model to model 4.  }

   (2) Merge the baseline models into one. We call it configural model. It represents the multi-group version of the baseline models. All the following models with some equality constraints on factor loadings will be compared with it. \textcolor{red}{See the first row of table 22 below, which tabulates the fit indices of configural model.}\
   (3) Constrain all the factor loadings equal across groups at one time and see how the model fits. Good fitting indicates multi-group in-variances in factor loadings. Bad fitting gives us evidence that there is some source of non-invariance, and we should find them. \textcolor{red}{See the second row of table 22 below, which tabulates the fit indices of a model with all factor loadings constrained equal across groups (the fit isn't good comparing to configural model so we can further search for source of non-invariance). }

   (4) Relax the constraint on factor loadings one parameter a time (according to the value of MIs of equality constrained parameters), and see how does the fit goes. More importantly, inspect the MLM chi square difference between the newly fitted model and (always) configural model. We want to repeat it until seeing non-significance, which indicates that except for the factor loadings we have relaxed from equality constraints, multi-group in-variance of factor loadings is proven. \textcolor{red}{See the third and fourth rows of table 22 below (especially take note of third column where p for chi-difference is presented), which demonstrates how our less and less restrictive models finally reach non-significant chi-square difference p value. }

   (5) Use the last model we established in last step (I will call it FL reference model) and, this time, further constrain equal across groups all the residual co-variances we have specified in configural model and fit it. Comparing this model to FL reference model, if the MLM chi-square difference yields non-significance, we can conclude that the multi-group in-variance assumption also holds for residual co-variances. \textcolor{red}{See table 23 below: residual-covariance-constrained model (lower) is compared to FL reference model (upper).}

   (5) Use the last model we established in last step (I will call it COV reference model) and, this time, further constrain equal across groups all the factor variances and co-variances and fit it. Comparing this model to COV reference model, if the MLM chi-square difference yields non-significance, we can conclude that the multi-group in-variance assumption also holds for factor variances and co-variances. \textcolor{red}{See table 24 below: factor-variance/covariance-constrained model (lower) is compared to COV reference model (upper).}

```{r}
key.table1;key.table2;key.table3;key.table4
```
