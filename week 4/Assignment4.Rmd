---
title: "COS-D419 Factor Analysis and Structural Equation Models 2023, Assignment 4"
author: "Rong Guang"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    latex_engine: lualatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F, cache = T)
```

# Task description

## Exercise 4.1

Draw the graph of the initial, full structural equation model. Make sure that you have included all the specified paths.

Estimate the initial model using the robust MLM estimator *(robust variant of the ML estimator, to be precise!)* and present a brief summary of the model fit.

## Exercise 4.2

Proceed **step by step** following the guidelines given in the lecture material, i.e., implement the modifications **one at a time**, testing and studying each step. See (and report) how the fit improves and which parameters are suggested to be modified. Please be careful! There will (always) be a lot of suggestions... Do not list all the MIs (only a few of them are useful!), try to keep your report as concise as possible.

## Exercise 4.3

Draw the graph of the final model and present its fit indices and the essential, standardized parameter estimates. **Pay attention to the factor correlations.**

Compare the initial and final graphs and make sure that you understand the whole modelling process and the final conclusions.

# Preparation

## Read in the data set

Start by downloading the data file from Moodle to Project folder.

```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(here, 
               expss, 
               tidyverse, 
               janitor,
               knitr, 
               qualtRics, 
               arules, 
               arulesViz, 
               sjlabelled,
               DT,
               stringr,
               rCBA,
               labelled)

library(tidyverse)
library(readr)
library(here)

#This week's file name
latest.name <- "ALLSEC.CSV"
#read in the data
mbi <- read_csv(file.path(here(),
                'data',
                latest.name))
```

## Write functions

To control length of reports, codes already shown in the previous homework were not showing in the current report. Yet they are available in .rmd report.

### to check unique values

```{r, echo = F}
unique.levels <-  function(sc){
  values <- lapply(sc, function(x)sort(unique(x)))
for(x in 1:ncol(sc)){
  a <- paste(c("Variable ",
               names(values)[x],
               " has values of ",
               paste(values[[x]],
                     collapse = ",")),
             collapse = "")
  print(a)
  }
}
```

### to generate CFA results with improved readability

```{r, echo = F}
library(kableExtra)
#goodness of fit indicators for ml
sem.summary.ml.a <- function(fit, fa.num, item.num, estimator){
  options(scipen = 999)
  cfa.measure <- fitMeasures(fit,    #obtain specified measured.
                            c("chisq",
                              "df",
                              "pvalue",
                              "cfi",
                              "tli",
                              "rmsea",
                              "rmsea.pvalue",
                              "srmr",
                              "chisq.scaling.factor"))
  names(cfa.measure) <- c("chi square", "df", "p value", "CFI", "TLI", "RMSEA", "RMSEA p value", "SRMR")
  #turn named vector to data frame
sem.tab.a <- cfa.measure %>%
  tibble(name= names(cfa.measure), value = cfa.measure) %>% # vector to df
  select(Measure = name, Value = value) %>%  #select and rename columns
  mutate(Value = round(as.numeric(Value),3)) %>%  # round
  kable(format = "markdown",   # table aesthetics
        booktabs = T, #Latex booktabs
        caption =  #caption
          paste("Goodness-of-fit and subjective indices of fit for", fa.num, "factor model ", estimator)) %>%
  kable_styling(latex_options = "striped") %>% # gray every other row
  row_spec(0, background = "#9999CC")
 cfa.tab.a
}
#goodness of fit indicators for mlm
sem.summary.mlm.a <- function(fit, fa.num, item.num, estimator,title){
  options(scipen = 999)
  cfa.measure <- fitMeasures(fit,    #obtain specified measured.
                            c("chisq.scaled",
                              "df.scaled",
                              "pvalue.scaled",
                              "cfi.scaled",
                              "tli.scaled",
                              "rmsea.scaled",
                              "rmsea.pvalue.scaled",
                              "srmr_bentler",
                              "chisq.scaling.factor"))
  names(cfa.measure) <- c("chi square", "df", "p value", "CFI", "TLI", "RMSEA", "RMSEA p value", "SRMR", "CSF")
  #turn named vector to data frame
cfa.tab.a <- cfa.measure %>%
  tibble(name= names(cfa.measure), value = cfa.measure) %>% # vector to df
  select(Measure = name, Value = value) %>%  #select and rename columns
  mutate(Value = round(as.numeric(Value),3)) %>%  # round
  kable(format = "markdown",   # table aesthetics
        booktabs = T, #Latex booktabs
        caption =  #caption
         title) %>%
  kable_styling(latex_options = "striped") %>% # gray every other row
  row_spec(0, background = "#9999CC")
 cfa.tab.a
}
#factor loading
sem.summary.b <- function(fit, fa.num, item.num, estimator){
  options(scipen = 999)
  #factor loading
  cfa.tab.b <- parameterEstimates(fit, standardized=TRUE) %>% # obtain estimates
  filter(op == "=~") %>%  #select "is measured by" rows
  select('Latent Factor'=lhs, #left hand side column
         Indicator=rhs, #right hand side column
         B=est, #estimates
         SE=se, #standard error
         Z=z, #z statistics
         'p-value'=pvalue, #p value
         Beta=std.all) %>%
  kable(digits = 3, #rounded to 3
        format="markdown", #Latex markdown
        booktabs=TRUE, #Latex booktabs
        caption=paste("Factor Loadings for",fa.num,"factor CFA model estimated by ", estimator)) %>% #caption
  kable_styling(latex_options = "striped") %>% #gray every other row
  row_spec(0, background = "#9999CC") # color the first row
  cfa.tab.b
  }
#Variance
sem.summary.c <- function(fit, fa.num, item.num, estimator){
  options(scipen = 999)
  #Variance
  type <- rep(c("Residual variance", "Total variance"),
            time = c(item.num, fa.num)) #create a new row clarifying types of variance
  
variance <- parameterEstimates(fit, standardized=TRUE) %>% #obtain estimates
  filter(op == "~~") #select "is correlated with" rows
variance <- variance[1:sum(item.num,fa.num),] #subset 1:18 rows (variance row)
variance <- cbind(type, variance) #add column
cfa.tab.c <- variance %>%select(Type = type, #select and rename variables
                   Indicator=rhs, #right hand side column
                   B=est, #estimates
                   SE=se,#standard error
                   Z=z, #z statistics
                   'p-value'=pvalue, #p value
                   Beta=std.all) %>%
  kable(digits = 3, #rounded
        format="markdown",  #Latex markdown
        booktabs=TRUE, #Latex booktabs
        caption=paste("Variances for", fa.num, "factor model estimated by ", estimator)) %>% #caption
  kable_styling(latex_options = "striped") %>% # gray every other row
  row_spec(0, background = "#9999CC") # color the variable row
  cfa.tab.c
}
#Covariance
sem.summary.d <- function(fit, fa.num, item.num, estimator){
  options(scipen = 999)
  #covariance
  variance <- parameterEstimates(fit, standardized=TRUE) %>%
  filter(op == "~~")
  covar.num = (fa.num+(fa.num-1))/2
variance <- variance[sum(item.num,fa.num,1):sum(item.num,fa.num,covar.num),]
type <- paste(variance$lhs, "with", variance$rhs)
variance <- cbind(type, variance)
rownames(variance) <- NULL
cfa.tab.d <- variance %>%select(Type=type,
                   B=est,
                   SE=se,
                   Z=z,
                   'p-value'=pvalue,
                   Beta=std.all) %>%
  kable(digits = 3,
        format="markdown",
        booktabs=TRUE,
        caption=paste("Covariances for", fa.num,
                      "factor model estimated by ",
                      estimator)) %>%
  kable_styling(latex_options = "striped") %>%
  row_spec(0, background = "#9999CC")
  cfa.tab.d
}
```

### to generate functions for improving aethetics of correlation matrix

```{r, echo = F}
library(GGally)
my.fun.density <- function(data, mapping, ...) { #notes are roughly same with above

    ggplot(data = data, mapping = mapping) +
       geom_histogram(aes(y=..density..),
                      color = "black",
                      fill = "white")+
       geom_density(fill = "#FF6666", alpha = 0.25) +
       theme(panel.grid.major = element_blank(),
             panel.grid.minor = element_blank(),
             panel.background = element_rect(fill = "#9999CC",
                                             color = "black"))
}
#define a function that allows me to fine-tune the matrix
my.fun.smooth <- function(data,    #my function needs 3 arguments
                          mapping,
                          method = "loess"){
  ggplot(data = data, #data is passed from ggpairs' arguments
         mapping = mapping)+#aes is passed from ggpairs' arguments
           geom_point(size = 0.3,  #draw points
                      color = "blue")+
           geom_smooth(method = method,  #fit a linear regression
                       size = 0.3,
                       color = "red")+
           theme(panel.grid.major = element_blank(), #get rid of the grids
                 panel.grid.minor = element_blank(),
                 panel.background = element_rect(fill = "#F0E442", #adjust background
                                                 color = "black"))
}
```

### to generate a function for histogram overlapping with density plot

```{r, echo = F}
corr.density <- function(data, fig.num = 1){
  data %>%
  pivot_longer(everything()) %>%  #longer format
  ggplot(aes(x = value)) + #x axis used variable "value" (a default of pivot)
  geom_histogram(binwidth = 1, aes(y = ..density..), #match ys of density and histogram plots
                 color = "black",  fill = "#9999CC")+  # adjust aesthetics for hist
  geom_density(fill = "pink", alpha = 0.25)+ #adjust aesthetics for density plot
  facet_wrap(~name, scales = "free", ncol =4) + #wrap by name variable
  theme(panel.grid.major = element_blank(), #get rid of the  grids
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "white",#adjust the background
                                        color = "black"),
        strip.background = element_rect(color = "black",#adjust the strips aes
                                        fill = "steelblue"),
        strip.text = element_text(size =8, color = "white"), #adjust strip text
        axis.title.x = element_text(size = 3), #adjust the x text
        axis.title.y = element_text(size = 3), # adjust the y text
        plot.title = element_text(size = 12,
                                  face = "bold",
                                  hjust = 0.5))+ #adjust the title
  labs(title = paste("Figure", fig.num," Distribution of selected items")) #title it
  }
```

### to generate a function for violin overlapping with box plot

```{r, echo = F}
violin.box <- function(data, fig.num = 2){
  mbi.long <- data %>% pivot_longer(everything(), names_to = "item", values_to = "score")

mbi.long %>%
  ggplot(aes(x = item, y = score)) +
  geom_violin(trim=F, fill = "#9999CC") +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust =1),
        axis.title = element_text(size = 12),
        panel.background = element_rect(fill = "white", color = "black"),
        plot.title = element_text(face="bold",
                                  hjust = 0.5),
        axis.title.x = element_blank())+
  labs(x = "Item",
       y = "Score",
       title = paste("Figureou", fig.num, " Violin plot of the selected items"))+
  geom_boxplot(width = 0.1, fill = "white")
}
```

### To generate a function describing continuous data set

```{r, echo=F}
descriptive <- function(data){
  library(finalfit)
library(kableExtra)
inspect.table <- ff_glimpse(data)$Continuous
inspect.table$label <- NULL
inspect.table %>%
  mutate('Q1Q3' = paste(quartile_25,
                        quartile_75,
                        sep = " ~ ")) %>%
  select(n,
         'n of NA' = missing_n,
         'Mean' = mean,
         'Median' = median,
         'SD' = sd,
         'Min' = min,
         'Max' = max,
         'Q1~Q3' = Q1Q3) %>%
  kable(booktabs = T,
        align = "r",
        longtable = T,
        linesep = "",
        caption = "Descriptive statistics for measurements") %>%
  add_header_above(c(" ",
                     " " = 2,
                     "Central tendency" = 2,
                     "Dispersion tendency" = 4)) %>%
  kable_styling(latex_options = c("striped",
                                  "repeat_header")) %>%
  column_spec(1, width = "3cm")
}
```

### Write a function to print a table with concerned parameters

```{r}
#write a function for minus calculation
minus <- function(x,y) {x - y}
#write a function to print a table with concerned parameters
concern.table <- function(sem, nofpath, model){
options(scipen = 999)
#This is for structural path residual variance
##regression path estimates
sem.parameter <- parameterEstimates(sem, standardized=TRUE) |>  # obtain estimates
  filter(op == "~") |>   #select "is measured by" rows
  mutate(Parameter = paste0(rhs, "→", lhs)) |>
  select(Parameter,
         'B'=est, #estimates
         'Beta'=std.all,#estimates standardized
         SE=se, #standard error
         Z=z, #z statistics
         'p-value'=pvalue #p value
         )

##round the p-value column
sem.parameter$`p-value` <- sem.parameter$`p-value` |>
  round(3)

##add a conditional logic to the p-value column that >0.05 cell shows in red
sem.parameter$`p-value` <- cell_spec(sem.parameter$`p-value`,
                                     color = ifelse(
                                       sem.parameter$`p-value` > 0.05,
                                       "red",
                                       "black")
                                     )

#This is for the residual variance of dependence variable
##obtain estimates
variance <- parameterEstimates(sem, standardized=TRUE)  |>
  filter(op == "~~") #select "is correlated with" rows
##subset needed rows (variance row)
variance <- variance |>
  filter(rhs %in% c("F8SELF", "F9ELC", "F10EE", "F11DP", "F12PA"))

#variance[minus(sum(32,nofpath), 5-1):sum(32,nofpath),] #32 is the n of indicators;
                                             #12 is the number of factors;
                                             #5 is the new of row I plan to show
##select&rename columns
sem.tab.variance <- variance |> select(
                   Parameter=rhs, #right hand side column
                   'B'=est, #estimates
                   'Beta'=std.all, #standardized estimates
                   SE=se,#standard error
                   Z=z, #z statistics
                   'p-value'=pvalue #p value
                   )
##remove the row names
rownames(sem.tab.variance) <- NULL
##round the p-value column
sem.tab.variance$`B` <- sem.tab.variance$`B` |>
  round(3)
##add a conditional logic to the p-value column that >0.05 cell shows in red
sem.tab.variance$`B` <- cell_spec(sem.tab.variance$`B`,
                                     color = ifelse(
                                       sem.tab.variance$`B` < 0,
                                       "red",
                                       "black")
                                     )
#bind the two table
concern.table <- rbind(sem.parameter, sem.tab.variance)
concern.table[1:nofpath, 2] <-
  as.character(round(as.numeric(concern.table[1:nofpath, 2]),3))
concern.table[sum(nofpath, 1):sum(nofpath, 5),6] <-
  as.character(round(
    as.numeric(
      concern.table[sum(nofpath,1):sum(nofpath, 5),6]
      ),
    3)
    )
#further aesthetics
concern.table |>
  select("Parameter*" = Parameter,
         'B†' = B, #estimates
         'Beta‡' = Beta,#estimates standardized
         SE, #standard error
         Z, #z statistics
         'p-value') |> #p value
  kable(digits = 3, #rounded to 3
        #format="latex", #Latex markdown
        booktabs=TRUE, #Latex booktabs
        linesep = "",
        align = "lrrrrr",
        caption=
          paste(
            "Residual variance of structural regression path and select factors for",
            model),
        escape = F) |> #caption
  kable_styling(latex_options = "striped") |> #gray every other row
  pack_rows("Strutural regression path",
            1,nofpath) |>
  pack_rows("Selected factors",
            sum(nofpath,1), sum(nofpath,5)) |>
  footnote(general = "Values highlighted in red should be taken note of",
           symbol = c("→ indicates regression path",
                      "Crude estimates",
                      "Standardized estimates"))
}
```

### To generate a function for calculating chi square difference was defined.

```{r, cache=T}
chi.diff <- function(sem1, sem2){ #2 augments input: the 1st is the nested model
                                  #the 2nd is comparison model (less restricted)
  measure0 <- fitMeasures(sem1,  #extract robust fit indices for the 1st model
                          c("df.scaled",
                            "chisq.scaling.factor",
                            "chisq.scaled")) |>
    as.vector()   #turn to vector to facilitate calculation
  measure1 <- fitMeasures(sem2,  #extract robust fit indices for the 2nd model
                          c("df.scaled",
                            "chisq.scaling.factor",
                            "chisq.scaled")) |>
    as.vector()
  cd <-  # this is the formula from p 14, week 4 slide
    (measure0[1]*measure0[2]-measure1[1]*measure1[2])/(measure0[1]-measure1[1])
  TRd <- # this is the formula from p 15, week 4 slide
    (measure0[3]*measure0[2]-measure1[3]*measure1[2])/cd
  TRd <- TRd |> round(3) # round the value to 3 decimal places
}
```

### To generate a function for ploting full sem diagram

```{r, fig.height =  10, fig.width = 14, cache=T}
full.sem.diagram <- function(model, nofpath, fig.num, quotedmodel){
semPaths(semPlotModel(model),
             style = "lisrel",
             rotation = 2,
             sizeLat = 6,
             sizeLat2 = 5,
             sizeMan = 5,
             sizeMan2 = 2,
             residScale = 4,
             shapeMan = "rectangle",
             edge.color = c(rep("black",34),
                            rep("blue", minus(nofpath,1)),
                            "orange", # this is the new free parameter
                            rep("gray",32),
                            rep("red",5)),
         residuals = T,
         layout = m,
         nCharNodes=0,
         optimizeLatRes = T,
         exoVar = F)
#add title
title(main = list(paste("Figure",
                        fig.num,
                        "Modified model (",
                        quotedmodel,
                        ") of teacher burnout"),
                  cex = 1.5, font =1),
     outer = F, line = -1)
#add notes
title(sub =
        "Notes: Orange arrow is the regression path set free to estimate in the current model",
      line = 0, adj = 0.7)
}
```

### To generate model comparison tables.

```{r}
model.comparison.table <- function(sem, mycaption, newpathstext){
  sem.compare <- matrix(NA, ncol =8, nrow = length(sem))
  
  for (i in 1:length(sem)){
    sem.compare[i,] <- fitMeasures(eval(parse(text=sem[i])),#obtain specified measured.
                            c("chisq.scaled",
                              "df.scaled",
                              "pvalue.scaled",
                              "cfi.scaled",
                              "tli.scaled",
                              "rmsea.scaled",
                              "srmr_bentler",
                              "chisq.scaling.factor")) |>
  round(3)
}
  colnames(sem.compare) <- c("chisq.scaled",
                              "df.scaled",
                              "pvalue.scaled",
                              "cfi.scaled",
                              "tli.scaled",
                              "rmsea.scaled",
                              "srmr_bentler",
                              "chisq.scaling.factor")
  
  sem.compare <- sem.compare |> data.frame()
  
#add column "chi square" and place values into it
    sem.compare$"ΔChi-square" <- rep(NA, length(sem))
    for (j in 2:length(sem)){ #calculate chisq difference
      sem.compare$"ΔChi-square"[j] <- chi.diff(eval(parse(text=sem[j-1])),
                                               eval(parse(text=sem[j])
                                                    )
                                               )
      }
#turn named vector to data frame
sem.compare<- sem.compare |> 
  select("Chi-square" = chisq.scaled, "DF"=df.scaled,
         "p value"=pvalue.scaled, "ΔChi-square", "CFI"= cfi.scaled,
         "TLI"= tli.scaled, "RMSEA"= rmsea.scaled,
         "SRMR"=srmr_bentler, "CSF"=chisq.scaling.factor)

#There is no chisq-diff value for the first model, so place a "--" in the cell
sem.compare[1,4] <- "--"

#add row names
    symbols <- c("1", "2*", "3†", "4‡", "5§", "6¶", "7**", "8††")
    for (x in 1:length(sem)){
      rownames(sem.compare)[x] <- paste0("Model", symbols[x]) 
      
    }
#table aesthetics fine-tune and print table
sem.compare |>
  kable(booktab =T,
        #format = "markdown",
        caption = mycaption,
        align = "r",
        linesep = "") |>
  kable_styling() |>
  footnote(symbol = newpathstext)
}
```

## Inspect the data

### Data structure

Have a quick overview of the data structure

```{r, cache=TRUE}
library(knitr)
library(broom)
dim(mbi);mbi %>% apply(2, function(x)class(x));
```

The data set contains 22 numeric variables of 372 obs. Their values appear to follow a consistent pattern covering the integer from 1 to 7, except for Items 4, 7, 17 and 21, which did not include a value of 1.

### Descriptive statistics of measured variables

```{r, cache=TRUE}
library(finalfit);library(kableExtra);

descriptive(mbi) |>
  pack_rows(index =
              c("Factor 1*: Role Ambiguity \n(high score means negative)" = 2,
                "Factor 2*: Role conflict \n(high score means negative)" = 2,
                "Factor 3*: Work overload \n(high score means negative)" = 2,
                "Factor 4‡: classroom climate" = 4,
                "Factor 5*: Decision-making" = 2,
                "Factor 6*: Superior support" =  2,
                "Factor 7*: Peer support" = 2,
                "Factor 8‡: Self-esteem" = 3,
                "Factor 9‡: External locus of control" = 5,
                "Factor 10†: Emotional Exhaustion \n(high score means negative)" = 3,
                "Factor 11†: Depersonalization \n(high score means negative)" = 2,
                "Factor 12†: Personal Accomplishment" = 3)) |>
  footnote(general =
             "Indicators variables were formulated through item parcels.",
    symbol = c("These indicators are parcels from Teacher Stress Scale instrument",
               "These indicators are parcels from BMI instrument",
               "These parcels consist of items from single unidimensional scales")
           )
```

### Visualization

(1) Histogram

Distribution of the data was examined via Histogram

```{r, fig.width = 7, fig.height=10, warning = F, message=F, cache=TRUE}
corr.density(mbi, fig.num = 1)
```

Ridge-line plots were generated for TSS and MBI indicators, respectively. By partially overlaying, it is a demonstration viable for comparing multiple distributions.

```{r, fig.height = 7, fig.width=8, warning = F, message=F, cache=TRUE}
library(ggridges)
library(viridis)
library(hrbrthemes)
library(patchwork)
a <- mbi |>
  select(
    starts_with("EE")|starts_with("DP")|starts_with("PA")
    ) |>
  pivot_longer(everything(), names_to = "variable", values_to = "value") |>
  ggplot(aes(x = value, y = variable, fill = ..x..)) +
  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +
  scale_fill_viridis(name = "parcelled score", option = "C") +
  labs(title =
         'Fig2 (a). Distribution of indicator scores from BMI instrument') +
  labs(x = "Indicator scores", y = "Indicators") +
    theme(
      legend.position="none",
      panel.spacing = unit(0.1, "lines"),
      plot.title = element_text(size = 12),
      panel.grid.major = element_blank(),
      panel.background = element_rect(color = "black",
                                      fill = "white")
      )

b <- mbi |>
  select(
    starts_with("ROL")|starts_with("WOR")|starts_with("DEC")|contains("SUP")
    ) |>
  pivot_longer(everything(), names_to = "variable", values_to = "value") |>
  ggplot(aes(x = value, y = variable, fill = ..x..)) +
  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +
  scale_fill_viridis(name = "parcelled score", option = "C") +
  labs(title =
         'Fig2 (b). Distribution of indicator scores from TSS instrument') +
  labs(x = "Indicator scores", y = "Indicators")+
    theme(
      legend.position="none",
      panel.spacing = unit(0.1, "lines"),
      plot.title = element_text(size = 12),
      panel.grid.major = element_blank(),
      panel.background = element_rect(color = "black",
                                      fill = "white")
    )

a/b
```

Clearly, within each instrument, indicators for same factor tend to show similar distribution features.

(2) Violin plot

Violin plot also provides information on distribution, plus ideas on out-liers.

```{r fig.width=10, fig.height=6, cache=TRUE}
a <- mbi |>
  select(1:16) |>
  violin.box(fig.num = "3(a)")

b <- mbi |>
  select(17:32) |>
  violin.box(fig.num = "3(b)")
library(patchwork)
a/b
```

(3) Correlation among items

```{r, fig.width = 7, fig.height=7, cache=TRUE}
#draw it
mbi |>  select(starts_with("EE")|starts_with("DP")|starts_with("PA")) |>
  ggpairs(lower =
          list(continuous = my.fun.smooth), #lower half show points with fitted line
        diag =
          list(continuous = my.fun.density), #diagonal grids show density plots
        title = "Fig4(a). Relationships among parcels of MBI instrument") + #title
  theme (plot.title = element_text(size = 12,  #adjust title visuals
                                   face = "bold"))
```

```{r, fig.width = 9, fig.height=9, cache = TRUE}
mbi |>  select(starts_with("ROL")|starts_with("WOR")|starts_with("DEC")|contains("SUP")) |>
  ggpairs(lower =
          list(continuous = my.fun.smooth), #lower half show points with fitted line
        diag =
          list(continuous = my.fun.density), #diagonal grids show density plots
        title = "Fig4(b). Relationships among parcels of TSS instrument") + #title
  theme (plot.title = element_text(size = 14,  #adjust title visuals
                                   face = "bold"))
```

# Testing the for the validity of causal structure of burnout

## Initial full structural equation model (hypothesized model modified according to CFA)

This full structural equation model was a hypothesized model. I have established causal structure linking several stressor variables considered to contribute to the presence of burnout (Fig. 5). These postulated causal relations linking variables were supported in theory and/or empirical research. I wanted to test the hypothesis that the causal pattern was true. The findings would contribute to the understanding of key determinants of teacher burnout. Since the hypothesis was proven not true, I performed post-hoc analysis to improve the causal structure step by step, until best fitting, albeit most parsimonious, model of any set of tested models had been achieved.

Note that *"an important preliminary step in the analysis of full latent variable models is to test first for the validity of the measurement model before making any attempt to evaluate the structural model. Accordingly, CFA procedures are used in testing the validity of the indicator variables. Once it is known that the measurement model is operating adequately, one can then have more confidence in findings related to assessment of the hypothesized structural model."* The current analysis started at the point where CFA had already been done. As described by Byrne, the analysis produced fit indices showing exceptionally good fit to the data; nonetheless, CFA model for the TSS was re-specified to include two additional parameters, both about allowing for cross-loading terms (DEC2 cross-loads onto Factor 1; DEC2 cross-loads onto Factor 5). These parameters set free were incorporated into the initial hypothesized model (Fig. 5).

### Define the initial model

```{r, fig.height =  16, fig.width = 28}
library(semPlot)#install.packages("semPlot")

initial_model <- '
# Factors:
 F1ROLA =~ ROLEA1 + ROLEA2 + DEC2
 F2ROLC =~ ROLEC1 + ROLEC2
 F3WORK =~ WORK1 + WORK2
 F4CLIM =~ CCLIM1 + CCLIM2 + CCLIM3 + CCLIM4
 F5DEC =~ DEC1 + DEC2
 F6SSUP =~ SSUP1 + SSUP2 + DEC2
 F7PSUP =~ PSUP1 + PSUP2
 F8SELF =~ SELF1 + SELF2 + SELF3
 F9ELC =~ ELC1 + ELC2 + ELC3 + ELC4 + ELC5
 F10EE =~ EE1 + EE2 + EE3
 F11DP =~ DP1 + DP2
 F12PA =~ PA1 + PA2 + PA3
# Regressions:
 F8SELF ~ F5DEC + F6SSUP + F7PSUP
 F9ELC ~ F5DEC
 F10EE ~ F2ROLC + F3WORK + F4CLIM
 F11DP ~ F2ROLC + F10EE
 F12PA ~ F1ROLA + F8SELF + F9ELC + F10EE + F11DP
'
```

### Visualize the initial model

To approximate the visual effect on slides, the coordinates for each nodes were defined on a 60 by 72 matrix.

```{r}
#generate a matrix
m <- matrix(NA, 60, 72)

#define positions of the factors
m[12, 68] <- "F1ROLA"
m[12, 40] <- "F2ROLC"
m[12, 28] <- "F3WORK"
m[12,12] <- "F4CLIM"
m[21,12] <-"F5DEC"
m[40,12] <-"F6SSUP"
m[53,9] <-"F7PSUP"
m[44,24] <-"F8SELF"
m[52,40] <-"F9ELC"
m[37,48] <-"F10EE"
m[26,60] <-"F11DP"
m[48,64] <-"F12PA"

#define the posiitions of the indicators (parcelled items)
m[4, 72] <- "ROLEA1"
m[4, 64] <- "ROLEA2"
m[4, 48] <- "ROLEC1"
m[4, 40] <- "ROLEC2"
m[4, 32] <- "WORK1"
m[4, 24] <- "WORK2"
m[4, 16] <- "CCLIM1"
m[5, 10] <- "CCLIM2"
m[10, 4] <- "CCLIM3"
m[15, 4] <- "CCLIM4"
m[20, 4] <- "DEC1"
m[27, 6] <- "DEC2"
m[36, 4] <- "SSUP1"
m[40, 4] <- "SSUP2"
m[59, 6] <- "PSUP1"
m[59, 13] <- "PSUP2"
m[48, 32] <- "SELF1"
m[52, 28] <- "SELF2"
m[51, 21] <- "SELF3"
m[56, 50] <- "ELC1"
m[60, 48] <- "ELC2"
m[60, 42] <- "ELC3"
m[60, 35] <- "ELC4"
m[56, 31] <- "ELC5"
m[43, 45] <- "EE1"
m[39, 40] <- "EE2"
m[35, 38] <- "EE3"
m[20, 64] <- "DP1"
m[20, 58] <- "DP2"
m[52, 71] <- "PA1"
m[56, 64] <- "PA2"
m[53, 57] <- "PA3"
```

The diagram of the initial model was generated.

```{r, fig.height =  10, fig.width = 14, cache=TRUE}
semPaths(semPlotModel(initial_model),
             style = "lisrel",
             rotation = 2,
             sizeLat = 6,
             sizeLat2 = 5,
             sizeMan = 5,
             sizeMan2 = 2,
             residScale = 4,
             shapeMan = "rectangle",
             edge.color = c(rep("black",34),
                            rep("blue",14),
                            rep("gray",32),
                            rep("red",5)),
         residuals = T,
         layout = m,
         nCharNodes=0,
         optimizeLatRes = T,
         exoVar = F)
title(main = list("Figure 5. Hypothesized model of teacher burnout",
                  cex = 1.5, font =1),
     outer = F, line = -1)
title(sub = "Notes: Red arrow indicates factor residuals; gray arrow indicates error residuals;
     blue arrow indicates regression path; black arrow indicates factor loading",
     line = 0, adj = 0.7)
```

### Estimate the SEM model (initial)

```{r}
library(lavaan)
model1 <- initial_model # defined above

# Estimate the model with the robust (MLM) estimator:
sem1 <- sem(model1, data = mbi, estimator = "MLM", mimic = "Mplus")

# Numerical summary of the model:
sem.summary.mlm.a(sem1, 12, 32, "mlm", "Model fit indices for initial model")
#summary(sem1, fit.measures = TRUE, standardized = TRUE)
```


```{r}
#print concern table for model 1
concern.table(sem1, 14, "model1")
```

### Comments on the result (initial model)

Here we see in tabel 2 that the re-scaled chi square value (i.e., the MLM chi square) is 1541.844 with 427 degrees of freedom. The reported chi square scaling factor value for the MLM estimator indicates that if the MLM chi square were multiplied by 1.127, it would approximate the uncorrected ML chi square value (1737.658).

Given the large number of parameters estimated in this model, the reported results are understandably lengthy. Thus, in the interest of space, I report findings pertinent to only the structural parameters, as well as a few residual variances. These results are presented in table 3.

Examination of estimated parameters in the model revealed all to be statistically significant except for those highlighted in red in table 3. These non-significant parameters the following structural regression paths: (a) F10 on F2 (Role Conflict → Emotional Exhaustion), F10 on F3 (Work Overload → Emotional Exhaustion), F10 on F4 (Classroom Climate → Emotional Exhaustion), and (b) F12 on F1 (Role Ambiguity → Personal Accomplishment).

For the negative variance for the residual associated with Factor 10 (highlighted in red in table 3), I left it as it was for the time being. The plan is over the progression of the addition of new regression path, the anamolies would disappear on itself.

### Model mis-specification

The sense-making for the choice of parameter to set free was placed in the table 4. The path regression to be set free in next model was highlighted in red, with the logics behind selecting it listing in the last column. 

A review of the MIs reveals some evidence of misfit in the model. Because we are interested solely in the causal paths of the model at this point, only MIs related to these parameters are included in table 5. 

```{r}
#extract needed variables
MI.model1 <- modindices(sem1,
                  standardized = TRUE,
                  sort. = TRUE,
                  maximum.number = 20) |>
  filter(op == "~") |>
  filter(lhs %in%  #When these factors are predicted variables, it is related
           c("F8SELF",  # to the topic of this study
             "F9ELC",
             "F10EE",
             "F11DP",
             "F12PA"))

#adapt to publication style
MI.model1 <- MI.model1 |>
  mutate(Parameter = paste(rhs, "→", lhs)) |>
  select(Parameter, MI = mi, EPC = epc, "std EPC" = sepc.all) |>
  filter(MI>10) |> #for saving space, the number of parameters was managed
  mutate("Logics" = c("Sensible and meaningful",
                      "illogical (wrong direction of correlation)",
                      "Sensibe but not meaningful for this study"))

#add footnote symbol to parameters
  for (i in 1:nrow(MI.model1)){
  symbol <- c("*", "†", "‡") #symbols for footnotes
  Parameter <- unlist(MI.model1$Parameter)
  MI.model1$Parameter[i] <-  paste0(Parameter[i], symbol[i])
  }

#Visualize the table
MI.model1 |>
  kable(digits = 3,
        booktab = T,
        linesep = "",
        caption = "Selected modification indices for initial model") |>
  kable_styling(latex_options = "striped") |>
  row_spec(1, color = "red") |> #this is the row with parameter to set free
  footnote(general =
             "Parameter highlighted in red is selected for modification",
    symbol = c("Poorer Classroom climate leads to worsening depersonalization",
             "Less workload leads to worsening depersonalization",
             "Higher accomplishment results in increased self-esteem"))
```


## Post hos analysis (Model 2)

### Compare SEM model 2 with initial model

I defined model 2 as per the conclusion from last section: set the regression path leading from F4 to F11 (Poorer Classroom climate leads to worsening depersonalization) free to estimate (Fig 6). The model fit indices and its comparison with the preceding model (model1) was tabulated in table 5.

```{r}
model2 <- paste(initial_model, "F11DP ~ F4CLIM")

# Estimate the model with the robust (MLM) estimator:
sem2 <- sem(model2, data = mbi, estimator = "MLM", mimic = "Mplus")
```


```{r}
sem <- c("sem1", "sem2")
newpathstext <-  " Model1 + structural path 'F4→F11' set free to estimate"

model.comparison.table (sem, 
                        mycaption = "Comparison of new and preceding models",
                        newpathstext = newpathstext)
```


```{r, fig.height =  10, fig.width = 14, cache=T}
#plot the diagram
full.sem.diagram(model2, 15, "6", "model2")
```

### Examine the parameters of concern for model 2

```{r}
#regression path estimates
concern.table(sem2, 15, "model2")
```

### Comment on the result (model 2)

The estimation of Model 2 yielded an overall MLM chi square(426) value of 1450.985 (scaling correction factor = 1.117); CFI and TLI values of 0.950 and 0.945, respectively; a RMSEA value of 0.041; and a SRMR value of 0.046 (Table 5). Although improvement in model fit for Model 2, compared with the originally hypothesized model, would appear to be somewhat minimal on the basis of the CFI, TLI, RMSEA, and SRMR values, the corrected chi-square difference test was found to be significant (MLM Δchi square[1] = 91.67), which finalized the decision (Table 5).

_*Notes from Byrne's book: "the thrust of these post-hoc analyses is to fine-tune our hypothesized structure such that it includes all viable and statistically significant structural paths, and, at the same time, eliminates all non-significant paths. Consequently, as long as the ∆chi square-difference test is statistically significant, and the newly added parameters are substantively meaningful, I consider the post-hoc analyses to be appropriate."*_

The anomaly of negative residual variance remained in the output for Model 2 (table 6). The estimated structural regression paths for the three factors hypothesized to influence Factor 10 (Factors 2, 3, and 4) and F1 to influence Factor 12 remained statistically non-significant (table 6).

Besides, The negative residual variance of F10 remained as it was in model 2 (table 6).

### Model mis-specification (model 2)

```{r}
#extract needed variables
MI.model2 <- modindices(sem2,
                  standardized = TRUE,
                  sort. = TRUE,
                  maximum.number = 25) |>
  filter(op == "~") |>
  filter(lhs %in%  #When these factors are predicted variables, it is related
           c("F8SELF",  # to the topic of this study
             "F9ELC",
             "F10EE",
             "F11DP",
             "F12PA"))

#adapt to publication style
MI.model2 <- MI.model2 |>
  mutate(Parameter = paste(rhs, "→", lhs)) |>
  select(Parameter, MI = mi, EPC = epc, "std EPC" = sepc.all) |>
  filter(MI>10) |>
  mutate("Logics" = c("Sensible but not meaningful for this study",
                      "Sensible and meaningful",
                      "Sensibe and meaningful, but MI is lower than F5→F12"))


#add footnote symbol to parameters
  for (i in 1:nrow(MI.model2)){
  symbol <- c("*", "†", "‡")
  Parameter <- unlist(MI.model2$Parameter)
  MI.model2$Parameter[i] <-  paste0(Parameter[i], symbol[i])
  }

#Visualize the table
MI.model2 |>
  kable(digits = 3,
        booktab = T,
        linesep = "",
        caption = "Selected modification indices for model 2") |>
  kable_styling(latex_options = "striped") |>
  row_spec(2, color = "red") |>
  footnote(general =
             "Parameter highlighted in red is selected for modification",
    symbol = c("Higher accomplishment leads to increased self-esteem",
             "Invovlement of more decision making gives sense of accomplishment",
             "People with high esteem will less likely get emotionally exhausted"))
```

## Post hos analysis (Model 3)

### Compare SEM model 3 with preceding models

I defined model 3 as per the conclusion from last section: on the basis of model 2, I set the regression path leading from F5 to F12 (Invovlement of more decision making gives sense of accomplishment) free to estimate (Fig 7). The model fit indices and their comparison with the preceding models were tabulated in table 8.

```{r}
#add new freely estimated parameter to the preceding model
model3 <- paste(model2, "\nF12PA ~ F5DEC")

# Estimate the model with the robust (MLM) estimator:
sem3 <- sem(model3, data = mbi, estimator = "MLM", mimic = "Mplus")
```

```{r, fig.width=14, fig.height=10}
full.sem.diagram(model3, 16, "7", "model3")
```

```{r}
sem <- c("sem1", "sem2", "sem3")
newpathstext <-  c(" Model1 + structural path 'F4→F11' set free to estimate",
                      " Model2 + structural path 'F5→F12' set free to estimate")

model.comparison.table (sem, 
                        mycaption = "Comparison of new (model3) and preceding models",
                        newpathstext = newpathstext)
```

### Examine the parameters of concern for model 3

```{r}
concern.table(sem3, 16, "model3")
```


### Comment on the result (model 3)

Model 3 yielded an overall MLM chi square(425) value of 1406.517 (scaling correction factor = 1.117), with CFI = 0.952, TLI = 0.944, RMSEA = 0.040, and SRMR = 0.044. Again, the MLM chi square difference between Models 2 and 3 is statistically significant (∆MLM chi square[1] = 46.866), albeit differences in the other fit indices across Models 2 and 3 were once again minimal. See table 8.

As expected, the estimate for the newly incorporated path from Decision Making to Personal Accomplishment (F12 on F5) was found to be statistically significant. However, the three previous non-significant path remained p values>0.05 (two leading to F12; one leading to F10). Besides, The negative residual variance of F10 remained as it was in model 3 (see tab 9, estimate highlighted in red).

### Model mis-specification (model 3)

```{r}
#extract needed variables
MI.model3 <- modindices(sem3,
                  standardized = TRUE,
                  sort. = TRUE,
                  maximum.number = 50) |>
  filter(op == "~") |>
  filter(lhs %in%  #When these factors are predicted variables, it is related
           c("F8SELF",  # to the topic of this study
             "F9ELC",
             "F10EE",
             "F11DP",
             "F12PA"))

#adapt to publication style
MI.model3 <- MI.model3 |>
  mutate(Parameter = paste(rhs, "→", lhs)) |>
  select(Parameter, MI = mi, EPC = epc, "std EPC" = sepc.all) |>
  filter(MI>10) |>
  head(3) |>
  mutate("Logics" = c("Sensible and meaningful, but not as sensible as F2→F9",
                      "Highly sensible and meaningful",
                      "Sensible and meaningful, but MI < other 2"))


#add footnote symbol to parameters
  for (i in 1:nrow(MI.model3)){
  symbol <- c("*", "†", "‡")
  Parameter <- unlist(MI.model3$Parameter)
  MI.model3$Parameter[i] <-  paste0(Parameter[i], symbol[i])
  }

#Visualize the table
MI.model3 |>
  kable(digits = 3,
        booktab = T,
        linesep = "",
        caption = "Selected modification indices for model 3") |>
  kable_styling(latex_options = "striped") |>
  row_spec(2, color = "red") |>
  footnote(general =
             "Parameter highlighted in red is selected for modification",
    symbol = c("Lower self-esteem will causes higher emotional exhaustion",
             "Higher role conflict increases external locus of control",
             "Lower self-esteem leads to higher external locus of control"))
```

Quoted from Byrne, "Again, I believe it is worthwhile to note why two alternate MI values, close in value to the one chosen here, are considered to be inappropriate. I refer to results related to the structural paths of F10 on F8 (MI = 38.868) and of F9 on F8 (MI = 35.670). In both cases, the flow of causal direction is incorrect." I don't agree with her comments on this wrong causal direction. To me, it is fairly logical, though not perfectly straightforward, that Lower self-esteem will cause higher emotional exhaustion (F8→F10, negative sign of EPC), and that lower self-esteem leads to higher external locus of control (F8→F9, negative sign of EPC). See table 10. However, I agree with Bynre's decision on most appropriate parameter F2→F9 (Higher role conflict increases external locus of control). The causal effect for this parameter is most intuitive --role conflict is a type of external factors that has negative influence. Considering its MI (41.343) is so close to the highest seen in this model (43.412), substantive meaningfulness, in my opinion, should have the final say when statistics disagrees moderately. As such, I will continue with setting F2 → F9 free to estimate in model 4. See next section.

## Post hos analysis (Model 4)

### Compare SEM model 4 with preceding models

I defined model 4 as per the conclusion from last section: on the basis of model 3, I set the regression path leading from F2 to F9 (Higher role conflict increases external locus of control) free to estimate (Fig 8). The model fit indices and their comparison with the preceding models were tabulated in table 11.

```{r}
#add new freely estimated parameter to the preceding model
model4 <- paste(model3, "\nF9ELC ~ F2ROLC")

# Estimate the model with the robust (MLM) estimator:
sem4 <- sem(model4, data = mbi, estimator = "MLM", mimic = "Mplus")
```

```{r, fig.width=14, fig.height=10}
full.sem.diagram(model4, 17, "8", "model4")
```

```{r}
sem <- c("sem1", "sem2", "sem3", "sem4")
newpathstext <-  c(" Model1 + structural path 'F4→F11' set free to estimate",
                      " Model2 + structural path 'F5→F12' set free to estimate",
                      " Model3 + structural path 'F2→F9' set free to estimate")

model.comparison.table (sem, 
                        mycaption = "Comparison of new (model 4) and preceding models",
                        newpathstext = newpathstext)
```


### Examine the parameters of concern for model 4

```{r}
concern.table(sem4, 17, "model4")
```

### Comment on the result (model 4)

The estimation of Model 4 yielded a MLM chi square value of 1366.129 (scaling correction factor = 1.118) with 424 degrees of freedom. Values related to the CFI, TLI, RMSEA, and SRMR were .954, .946, 0.039, and 0.041, respectively. Again, the difference in fit between this model (Model 4) and its predecessor (Model 3) was statistically significant (MLM∆chi square[1] = 52.949). See table 11.

As expected, the newly specified parameter (F9 on F2) was found to be statistically significant (Estimate = 0.189; Z-statistics = 7.157). However, once again the three paths leading from F2, F3, and F4 to F10, and from F10 to F12 were all found to be non-significant. Finally, once again, the negative residual associated with Factor 10 appeared. See table 12. 

### Model mis-specification

```{r}
#extract needed variables
MI.model4 <- modindices(sem4,
                  standardized = TRUE,
                  sort. = TRUE,
                  maximum.number = 50) |>
  filter(op == "~") |>
  filter(lhs %in%  #When these factors are predicted variables, it is related
           c("F8SELF",  # to the topic of this study
             "F9ELC",
             "F10EE",
             "F11DP",
             "F12PA"))

#adapt to publication style
MI.model4 <- MI.model4 |>
  mutate(Parameter = paste(rhs, "→", lhs)) |>
  select(Parameter, MI = mi, EPC = epc, "std EPC" = sepc.all) |>
  filter(MI>10) |>
  head(3) |>
  mutate("Logics" = c("Sensible and meaningful",
                      "Sensible but not meaningful for the study",
                      "Sensible and not meaningful for the study"))


#add footnote symbol to parameters
  for (i in 1:nrow(MI.model4)){
  symbol <- c("*", "†", "‡")
  Parameter <- unlist(MI.model4$Parameter)
  MI.model4$Parameter[i] <-  paste0(Parameter[i], symbol[i])
  }

#Visualize the table
MI.model4 |>
  kable(digits = 3,
        booktab = T,
        linesep = "",
        caption = "Selected modification indices for model 4") |>
  kable_styling(latex_options = "striped") |>
  row_spec(1, color = "red") |>
  footnote(general =
             "Parameter highlighted in red is selected for modification",
    symbol = c("Lower self-esteem causes severer external locus of control",
             "Higher personal accomplishment increases self-esteem",
             "Higher personal accomplishment decreases external locus of control"))
```

The regression path F8→F9 shows up again but this time it has the highest MI among its peers. See table 13. In the preceding model (model 3) I did not adopt it only because its relatively lower MI albeit good sensibility and meaningfulness. Given that other two competitor parameters in the current model were bad in meaningfulness (because they regress risk factors on the outcome of interest, but we want the opposite), we could decide on adopting the path of F8→F9 (Lower self-esteem causes severer external locus of control) without much ado.


## Post hos analysis (Model 5)

### Compare SEM model 5 with preceding models

I defined model 5 as per the conclusion from last section: on the basis of model 4, I set the regression path leading from F8 to F9 (Lower self-esteem causes severer external locus of control) free to estimate (Fig 9). The model fit indices and their comparison with the preceding models were tabulated in table 14.

```{r}
#add new freely estimated parameter to the preceding model
model5 <- paste(model4, "\nF9ELC ~ F8SELF")

# Estimate the model with the robust (MLM) estimator:
sem5 <- sem(model5, data = mbi, estimator = "MLM", mimic = "Mplus")
```

```{r, fig.width=14, fig.height=10}
full.sem.diagram(model5, 18, "9", "model5")
```

```{r}
sem <- c("sem1", "sem2", "sem3", "sem4", "sem5")
newpathstext <-  c(" Model1 + structural path 'F4→F11' set free to estimate",
                      " Model2 + structural path 'F5→F12' set free to estimate",
                      " Model3 + structural path 'F2→F9' set free to estimate",
                      " Model5 + structural path 'F8→F9' set free to estimate")

model.comparison.table (sem, 
                        mycaption = 
                          "Comparison of new  (model 5) and preceding models",
                        newpathstext = newpathstext)
```


### Examine the parameters of concern for model 5

```{r}
concern.table(sem5, 18, "model5")
```

### Comment on the result (model 5)

The estimation of Model 5 yielded a MLM chi square(423) value of 1331.636 (scaling correction factor = 1.117), with other fit indices as follows: CFI = 0.955, TLI = 0.948, RMSEA = 0.039, and SRMR = 0.039. Again, the difference in fit between this model (Model 5) and its predecessor (Model 4) was found to be statistically significant (MLM ∆chi square[1] = 29.877). See table 14. 

Once again, the newly specified parameter (F9 on F8) was found to be statistically significant and accompanied by the correct sign (Estimate = -0.257; Z-statistics = -5.337). However, as with Model 4, again two of the three paths leading to F10 (regressions of F2 → F10; F4 → F10), and one leading from F10 to F12 remained statistically non-significant. Besides, the negative residual associated with Factor 10 remained unchanged. See table 15.

### Model mis-specification

```{r}
#extract needed variables
MI.model5 <- modindices(sem5,
                  standardized = TRUE,
                  sort. = TRUE,
                  maximum.number = 50) |>
  filter(op == "~") |>
  filter(lhs %in%  #When these factors are predicted variables, it is related
           c("F8SELF",  # to the topic of this study
             "F9ELC",
             "F10EE",
             "F11DP",
             "F12PA"))

#adapt to publication style
MI.model5 <- MI.model5 |>
  mutate(Parameter = paste(rhs, "→", lhs)) |>
  select(Parameter, MI = mi, EPC = epc, "std EPC" = sepc.all) |>
  head(2) |>
  mutate("Logics" = c("Sensible and meaningful",
                      "Sensible but not meaningful for the study"))

#add footnote symbol to parameters
  for (i in 1:nrow(MI.model5)){
  symbol <- c("*", "†", "‡")
  Parameter <- unlist(MI.model5$Parameter)
  MI.model5$Parameter[i] <-  paste0(Parameter[i], symbol[i])
  }

#Visualize the table
MI.model5 |>
  kable(digits = 3,
        booktab = T,
        linesep = "",
        caption = "Selected modification indices for model 5") |>
  kable_styling(latex_options = "striped") |>
  row_spec(1, color = "red") |>
  footnote(general =
             "Parameter highlighted in red is selected for modification",
    symbol = c("Lower self-esteem causes severer emotional exhaustion",
             "Higher depersonalization causes severer role conflict"))
```

This output reveals the structural path leading from Self-Esteem to Emotional Exhaustion (F8 → F10) as having the largest MI value. Given that the fact that it seems reasonable that teachers who exhibit high levels of self-esteem may exhibit low levels of emotional exhaustion, the model was re-estimated once again, with this path freely estimated. See tab .One other interesting reason to do so, cited from Byrne is that _"Because Factor 10 has been problematic regarding the estimation of its residual in yielding a negative variance, it would seem likely that if this parameter were to be included in the model, this undesirable result may finally be resolved."_

## Post hos analysis (Model 6)

### Compare SEM model 6 with preceding models

I defined model 6 as per the conclusion from last section: on the basis of model 5, I set the regression path leading from F8 to F9 (Lower self-esteem causes severer emotional exhaustion) free to estimate (Fig 10). The model fit indices and their comparison with the preceding models were tabulated in table 17.

```{r}
#add new freely estimated parameter to the preceding model
model6 <- paste(model5, "\nF10EE ~ F8SELF")

# Estimate the model with the robust (MLM) estimator:
sem6 <- sem(model6, data = mbi, estimator = "MLM", mimic = "Mplus")
```

```{r, fig.width=14, fig.height=10}
full.sem.diagram(model6, 19, "10", "model6")
```

```{r}
sem <- c("sem1", "sem2", "sem3", "sem4", "sem5", "sem6")
newpathstext <-  c(" Model1 + structural path 'F4→F11' set free to estimate",
                      " Model2 + structural path 'F5→F12' set free to estimate",
                      " Model3 + structural path 'F2→F9' set free to estimate",
                      " Model5 + structural path 'F8→F9' set free to estimate",
                      " Model6 + structural path 'F8→F10' set free to estimate")

model.comparison.table (sem, 
                        mycaption = 
                          "Comparison of new  (model 6) and preceding models",
                        newpathstext = newpathstext)
```

### Examine the parameters of concern for model 6

```{r}
concern.table(sem6, 19, "model6")
```

### Comment on the result (model 6)

The estimation of Model 6 yielded a MLM chi square(422) value of 1297.489 (scaling correction factor = 1.116), with the remaining fit indices as follows: CFI = 0.957, TLI = 0.949, RMSEA = 0.038, and SRMR = 0.039. As might be expected given the newly specified path involving Factor 10, the difference in fit between this model (Model 6) and its predecessor (Model 5) was found to be statistically significant (MLM∆chi square[1] = 23.358). See tab 17.

An important finding here is that the alarming negative residual variance of Factor 10 finally disappeared. The estimated value for the newly specified path (F10 on F8) was statistically significant (Estimate = −0.884; Est/SE = −8.116). Furthermore, one of the previously determined non-significant paths loading from F4 onto F10 turned significant. Nonetheless, the regression path leading from F10 to F12, which has been consistently non-significant, remained so. In addition, however, the parameter, F8 on F7 (Peer Support → Self-Esteem), was found to be statistically non-significant. See tab 18.

### Model mis-specification

```{r}
#extract needed variables
MI.model6 <- modindices(sem6,
                  standardized = TRUE,
                  sort. = TRUE,
                  maximum.number = 50) |>
  filter(op == "~")

#adapt to publication style
MI.model6 <- MI.model6 |>
  mutate(Parameter = paste(rhs, "→", lhs)) |>
  select(Parameter, MI = mi, EPC = epc, "std EPC" = sepc.all) |> 
  mutate("Logics" = c("Sensible but not meaningfulfor the study",
                      "Not sensible",
                      "Sensible and meaningful"))

#add footnote symbol to parameters
  for (i in 1:nrow(MI.model6)){
  symbol <- c("*", "†", "‡")
  Parameter <- unlist(MI.model6$Parameter)
  MI.model6$Parameter[i] <-  paste0(Parameter[i], symbol[i])
  }

#Visualize the table
MI.model6 |>
  kable(digits = 3,
        booktab = T,
        linesep = "",
        caption = "Selected modification indices for model 6") |>
  kable_styling(latex_options = "striped") |>
  row_spec(3, color = "red") |>
  footnote(general =
             "Parameter highlighted in red is selected for modification",
    symbol = c("Higher depersonalization causes lower self-esteem",
               "Higher role conflict improves personal accomplishment",
               "Higher self-esteem causes lower depersonalization"
             ))
```

Interestingly, Byrne's comments on F8→F11 is that _"the parameter represents a causal path flowing from F8 (Self-Esteem) to F11 (Depersonalization), and the fact that the EPC statistic has a negative sign makes interpretation of this path illogical as it conveys the notion that high self-esteem leads to low levels of depersonalization."_ However, I was not able to make sense out of it since it seems to me that Higher self-esteem (a positive perception) lowers depersonalization (a negative perception) is a plausible and meaningful relation. People with strong confidence in themselves would less likely to feel being detached and numbness. Futhermore, Byrne believed F2→F12 (Higher role conflict improves personal accomplishment) "makes sense substantively", which I was not able to grasp the idea. Actually, I believed this regression path is the most appropriate candidate for modification in current model, given the one with highest MI (F11→F8, Higher depersonalization causes lower self-esteem) was sensible but not digging into the problem under scope of the present study (We want it opposite). As such, at the point I decided to go for a different track than Byrne by modifying F8→F11 instead of F2→F12. See tab 19.

## Post hos analysis (Model 7)

### Compare SEM model 7 with preceding models

I defined model 7 as per the conclusion from last section: on the basis of model 6, I set the regression path leading from F8 to F11 (Higher role conflict improves personal accomplishment) free to estimate (Fig 11). The model fit indices and their comparison with the preceding models were tabulated in table 20.

```{r}
#add new freely estimated parameter to the preceding model
model7 <- paste(model6, "\nF11DP ~ F8SELF")

# Estimate the model with the robust (MLM) estimator:
sem7 <- sem(model7, data = mbi, estimator = "MLM", mimic = "Mplus")
```

```{r, fig.width=14, fig.height=10}
full.sem.diagram(model7, 20, "11", "model7")
```

```{r}
sem <- c("sem1", "sem2", "sem3", "sem4", "sem5", "sem6", "sem7")
newpathstext <-  c("Model1 + structural path 'F4→F11' set free to estimate",
                      "  Model2 + structural path 'F5→F12' set free to estimate",
                      "  Model3 + structural path 'F2→F9' set free to estimate",
                      "  Model5 + structural path 'F8→F9' set free to estimate",
                      "  Model6 + structural path 'F8→F10' set free to estimate",
                      "Model7 + structural path 'F2→F12' set free to estimate")

model.comparison.table (sem, 
                        mycaption = 
                          "Comparison of new  (model 7) and preceding models",
                        newpathstext = newpathstext)
```

### Examine the parameters of concern for model 7

```{r}
concern.table(sem7, 20, "model7")
```

### Comments on the results(model 7)

The estimation of Model 7 yielded a MLM chi square(421) value of 1267.208 (scaling correction factor = 1.116), with the remaining fit indices as follows: CFI = 0.958, TLI = 0.951, RMSEA = 0.038, and SRMR = 0.038. Again, the difference in fit between this model (Model 7) and its predecessor (Model 6) was found to be statistically significant (MLM∆chi square[1] = 23.057). See tab 20.

Here again, the new parameter added to the model (F11 on F8) was found to be statistically significant (Estimate = -0.42; Z-statistics = -4.688). However, the structural regression paths F8 (Self-Esteem) on F7 (Peer Support) and F12 (Personal Accomplishment) on F10 (Emotional exhaustion) remained non-signification. See tab 21.

At this point, I believe that I have exhausted the search for a better fitting model that best represents the data for secondary teachers. Thus, I start to delete regression path that are originally specified in the hypothesized model but remain statistically non-significant as per the results for Model 7. Accordingly, Model 8 is specified in which the two non-significant parameters noted above (F7 → F8; F10 → F12) are not included. Note that one of the excluded paths is different from Byrne's decision, where she deleted F12 on F1 instead of F12 on F10. See next section.

## Final Model

### Estimate the final model (model 8)

```{r}
#add new freely estimated parameter to the preceding model
model8 <- '
    F1ROLA =~ ROLEA1 + ROLEA2 + DEC2
    F2ROLC =~ ROLEC1 + ROLEC2
    F3WORK =~ WORK1 + WORK2
    F4CLIM =~ CCLIM1 + CCLIM2 + CCLIM3 + CCLIM4
    F5DEC =~ DEC1 + DEC2
    F6SSUP =~ SSUP1 + SSUP2 + DEC2
    F7PSUP =~ PSUP1 + PSUP2
    F8SELF =~ SELF1 + SELF2 + SELF3
    F9ELC =~ ELC1 + ELC2 + ELC3 + ELC4 + ELC5
    F10EE =~ EE1 + EE2 + EE3
    F11DP =~ DP1 + DP2
    F12PA =~ PA1 + PA2 + PA3
    F8SELF ~ F5DEC + F6SSUP 
    F9ELC ~ F5DEC
    F10EE ~ F2ROLC + F3WORK + F4CLIM
    F11DP ~ F2ROLC + F10EE
    F12PA ~ F1ROLA + F8SELF + F9ELC + F11DP
    F11DP ~ F4CLIM
    F12PA ~ F5DEC
    F9ELC ~ F2ROLC
    F9ELC ~ F8SELF
    F10EE ~ F8SELF
    F12PA ~ F2ROLC
'

# Estimate the model with the robust (MLM) estimator:
sem8 <- sem(model8, data = mbi, estimator = "MLM", mimic = "Mplus")
#summary(sem8, fit.measures = T, standardized =T)
```

I established the final model (model 8) by deleting two paths (F7 → F8; F10 → F12) from model 7. See picture 12

Estimation of this final model (Model 8) yielded an overall MLM chi square(423) value of 1291.217 (scaling correction factor = 1.116). This 423 degrees of freedom seems very suspicious since Byrne's final model also had 423 degrees of freedom. However, my final model deleted one less factor than hers. My intuition is the degree of freedom could be different too. Hence, I tried to manually count of the df of my final model to see if there was anything going wrong. 

*My final number of manifest variable was 32. Hence I had 32*33/2=528 pieces of information*

*I had 95 estimated parameters:*

  *22 factor loadings ;*
  *44 variances(7 factors, 32 error residual, 5 factor residual);*
  *21 factor covariances (factor 1-7)*
  *18 structural regression paths.*
  
*So I have 528 - (22+44+21+18) = 423 degrees of freedom.*

So nothing went wrong. 

The final model have a CFI value of 0.957, representing a very good fit to the data for secondary teachers. The remaining TLI, RMSEA, and SRMR values further substantiate these results. In contrast to all previous models, it is also found that the corrected chi-square difference between the present model and Model 7 to be statistically significant (MLM∆chi square[1] = -14.863). See table 22. 

It is also interesting to test the corrected chi-square difference between the final model and initial model:

```{r}
print(chi.diff(sem1,sem8))
```

A chi square value of 240.081 indicated significant difference. The final model fit the data significantly better than the initial model.

```{r}
sem <- c("sem1", "sem2", "sem3", "sem4", "sem5", "sem6", "sem7", "sem8")
newpathstext <-  c(" Model1 + structural path 'F4→F11' set free to estimate",
                      "  Model2 + structural path 'F5→F12' set free to estimate",
                      "  Model3 + structural path 'F2→F9' set free to estimate",
                      "  Model5 + structural path 'F8→F9' set free to estimate",
                      "  Model6 + structural path 'F8→F10' set free to estimate",
                      "Model7 + structural path 'F2→F12' set free to estimate",
                      "Model8 - structural path with F7 → F8 and F10 → F12 set fixed")

model.comparison.table (sem, 
                        mycaption = 
                          "Comparison of final(model 8) and preceding models",
                        newpathstext = newpathstext)
```


```{r}
#broom cells 
m[53,9] <- NA
m[59,6] <- NA
m[59,13] <- NA
```

```{r}
model.plot <- '
    F1ROLA =~ ROLEA1 + ROLEA2 + DEC2
    F2ROLC =~ ROLEC1 + ROLEC2
    F3WORK =~ WORK1 + WORK2
    F4CLIM =~ CCLIM1 + CCLIM2 + CCLIM3 + CCLIM4
    F5DEC =~ DEC1 + DEC2
    F6SSUP =~ SSUP1 + SSUP2 + DEC2
    F8SELF =~ SELF1 + SELF2 + SELF3
    F9ELC =~ ELC1 + ELC2 + ELC3 + ELC4 + ELC5
    F10EE =~ EE1 + EE2 + EE3
    F11DP =~ DP1 + DP2
    F12PA =~ PA1 + PA2 + PA3
    F8SELF ~ F5DEC + F6SSUP 
    F9ELC ~ F5DEC
    F10EE ~ F2ROLC + F3WORK + F4CLIM
    F11DP ~ F2ROLC + F10EE
    F12PA ~ F1ROLA + F8SELF + F9ELC + F11DP
    F11DP ~ F4CLIM
    F12PA ~ F5DEC
    F9ELC ~ F2ROLC
    F9ELC ~ F8SELF
    F10EE ~ F8SELF
    F12PA ~ F2ROLC
'
# Estimate the model with the robust (MLM) estimator:
sem.plot <- sem(model.plot, data = mbi, estimator = "MLM", mimic = "Mplus")
#summary(sem8, fit.measures = T, standardized =T)

```

### Visualize the final model

Figure 12 displays the final model, which highlights in orange all the regression paths added in addition to the initial model. Figure 5(the initial model) is plotted again under figure 12 to make comparison easy. We can see in comparison to initial model, the final model incorporates 5 more structural regression paths, and deleted one factor (with its two indicators) and two structural regression paths(F8 on F7 and F12 on F10). Among the 5 additional paths, two leads from TSS sub-scales into BMI sub-scales (F12-personal accomplishment on F2-role conflict; F12 on F5-decision making); one from self-esteem scale (uni-dimension) to BMI sub-scale (F10, emotional exhaustion); one from TSS sub-scale (F2, role conflict) to external locus of control scale (uni-dimension); and one from self-efficacy scale to external locus of control scale. The deleted factor (Peer support) belong to TSS scale. One deleted regression path leads from peer support to self-esteem scale; the other deleted path leads from one MBI sub-scale (F12-personal accomplishment) to another (F10 emotional exhaustion). 

```{r, fig.width=12, fig.height=16}
par(mfrow=c(2,1))
a <- semPaths(semPlotModel(model.plot),
             style = "lisrel",
             rotation = 2,
             sizeLat = 6,
             sizeLat2 = 5,
             sizeMan = 5,
             sizeMan2 = 2,
             residScale = 4,
             shapeMan = "rectangle",
             edge.color = c(rep("black",32),
                            rep("blue",13),
                            rep("orange",5),
                            rep("gray",32),
                            rep("red",5)),
         residuals = T,
         layout = m,
         nCharNodes=0,
         optimizeLatRes = T,
         exoVar = F)
title(main = list("Figure 12. Final model of teacher burnout",
                  cex = 1.5, font =1),
     outer = F, line = -1)
title(sub = "Notes: Red arrow indicates factor residuals; gray arrow indicates error residuals;
      black arrow indicates factor loading; blue arrow indicates originally hypothesized 
      regression path; orange arrow indicates regression paths added over pos hoc analysis",
     line = 1, adj = 0.7)

m[53,9] <-"F7PSUP"
m[59, 6] <- "PSUP1"
m[59, 13] <- "PSUP2"

semPaths(semPlotModel(initial_model),
             style = "lisrel",
             rotation = 2,
             sizeLat = 6,
             sizeLat2 = 5,
             sizeMan = 5,
             sizeMan2 = 2,
             residScale = 4,
             shapeMan = "rectangle",
             edge.color = c(rep("black",34),
                            rep("blue",14),
                            rep("gray",32),
                            rep("red",5)),
         residuals = T,
         layout = m,
         nCharNodes=0,
         optimizeLatRes = T,
         exoVar = F)
title(main = list("Figure 5. Hypothesized (Initial) model of teacher burnout",
                  cex = 1.5, font =1),
     outer = F, line = -1)
title(sub = "Notes: Red arrow indicates factor residuals; gray arrow indicates error residuals;
     blue arrow indicates regression path; black arrow indicates factor loading",
     line = 0, adj = 0.7)
```



```{r, fig.width=14, fig.height=10}

grps <- list(c("F10EE","F11DP","F12PA"),
             c("F1ROLA","F2ROLC","F3WORK","F4CLIM","F5DEC","F6SSUP","F8SELF","F9ELC"))
a <- "Figure 13. Summary of final model structural paths representing determinants 
      of burnout for teachers"
#broom cells 
m[53,9] <- NA
m[59,6] <- NA
m[59,13] <- NA
m[40,12] <-NA
m[53,9] <-"F6SSUP"
semPaths(sem8,
         style = "lisrel",
         #"path",
         #"par",
         "std",
             rotation = 2,
             sizeLat = 10,
             sizeLat2 = 7,
             sizeMan = 5,
             sizeMan2 = 2,
             residScale = 4,
             shapeMan = "rectangle",
             #edge.color = c(rep("black",5),
              #              rep("blue",5),
                #            rep("gray",4)),
         residuals = F,
         layout = m,
         nCharNodes=0,
         optimizeLatRes = T,
         exoVar = F,
         covAtResiduals = F,
         intercepts = F,
         exoCov = F,
         structural = T,
         posCol = c("red","red"),
         negCol = c("steelblue","steelblue"),
         fade = F,
         edge.label.cex = 0.7,
         group =grps,
         color = c("lightblue", "white")
)
title(main = list(a,
                  cex = 1.5, font =1),
     outer = F, line =-1)
title(sub = "Notes: Nodes in light blue are outcome of interest;
             Red edge indicates positive influence; blue edges indicate negative influence;
             The thickness of the line indicates strength of influence",
     line = 0, adj = 0.7)
```

The theoretical structure of the final model is shown in figure 13, describing how the psychological determinants influencing teacher's burnout. Standardized parameter estimates are shown on the edges. We can see high workload is one important risk factor to emotional exhaustion, while people with high self-esteem are less likely to develop emotional exhaustion. And within the structure of burnout, the mechanism reflected by the study is--under external influence, teachers start to experience emotional exhaustion. Accumulated, they will feel depersonalized, which further decreases their perception of personal accomplishment. Some other risk and protective factors to burnout can be easily found via this diagram.

However, with the aide of this picture, I saw some anomalies that I missed in the preceding procedures (Actually, I found it after I have submitted the first version of my homework). My final model shows that Role Conflict(F2), even as a negative perception, alleviates Emotional Exhaustion (F10) and facilitates Personal Accomplishment (F12) at the same time. This is so counter-intuitive. By examining slides and Byrne's book, I found these strange effects were also present in their final model (even though my final model and theirs are different). If I had to justify it, it would be that people who are young and just enter a job are more subject to role conflict issues and, concomitantly, they are also the group of people who are energetic (so somewhat immune to exhaustion) and poised to struggle for accomplishment. Yet, if I could continue with this analysis, I would try going through more models to see if these anomalies disappear or simply delete role conflict from the model. *This raises the old question again that when should we stop modifying the model, and also raises a new question that to which extent we should decide base on intuition, (unproven) theory or sense-making, the  counter-proof of which are not rare in science (this might be why we follow evidence-based medicine, instead of theory-based medicine)*. As such, I decided to stop with going over more post-hoc studies and, instead, try to do some more exploration. 

# More exploration

## I will explore two intertwined questions:

&nbsp;&nbsp;&nbsp;If we keep doing post-hoc analyses step by step for multiple rounds, how would the model fits change over these rounds?

&nbsp;&nbsp;&nbsp;If we keep doing the above post-hoc analyses in a judicious sense-making manner, will the model fits really out-perform those by sheer statistical procedure?

## To answer these questions, the plan is:

&nbsp;&nbsp;&nbsp;The goodness of model fit will be measured by fit indices (and other indicators) from a test data set (in contrast to a train data set. 50/50 splits of MBI data);

&nbsp;&nbsp;&nbsp;The error of model fit will be captured by a procedure bearing similarity with bootstrapping. Specifically, the train/test will be randomly split for 60 times. For each train/test set, the SEM model will be estimated for 20 modifications, respectively. However, the regression path to set free in each step for both train and test datasets will be determined singly by the MI of train set. 

## This will be done in an automated way:

&nbsp;&nbsp;&nbsp;All the procedure above will be executed twice. They are same in every aspect except for the procedures by which the regression paths to be set free are selected. One selects paths by sheer statistical numbers (Always select the path with largest MI); the other selects the paths in a rule-based way. The rule-based path selection will also be automated. Yet it will, to my best efforts, approximate the implicit logics that Byrne used for the example in our exercise. They are: 

&nbsp;(1)each candidate regression path will be examined in two aspects: a. Logics and b. study focus.

&nbsp;(2)To measure logics, all the 11 factors will be assigned as positive or negative perception according to what Byrne had explicitly defined in the chapter. The reasoning is a.When a factor leads to another factor with the same positive/negative direction with it, the EPC should >0; b. When a factor leads to another factor with different positive/negative direction with it, the EPC should <0. All the parameters obey these rules will be seen as logical; all the parameters violate these rules will be taken as illogical.

&nbsp;(3)Based on Byrne, a regression path that satisfied the study focus could be defined as a logical path in which the dependent variable is one of the three indicators from BMI inventory (EE, DP, PA). 

&nbsp;(4) In the automation, R first loop through all the rows of MI table to find regression path that are both logical and of study focus. The first parameter that meets the criteria will be chosen for modification; If no one parameter is found to be competent until the last row, R will loop through all the rows again for the parameter that is logical (but couldn’t be of study focus), and the first one that meets this criterion will be chosen for modification.

&nbsp;(5) Note that the sheer statistical selecting procedure also had a very minimal logic that excludes all the paths that lead from the BMI indicators to other indicators outside BMI. 

## Execute the plan

### Write a function that automate a rule-based judicious select of regression path to set free 

```{r}
find.path <- function(mi.train){ #augment is an MI table that only contains "~" term
  logic <- c(-1,-1,-1,1,1,1,1,1,1,-1,-1,1) #define the positive/negative direction 
  names(logic) <- c("F1ROLA","F2ROLC","F3WORK","F4CLIM", #for each indicator
                  "F5DEC","F6SSUP","F7PSUP","F8SELF",
                  "F9ELC","F10EE","F11DP","F12PA")
  
  interest <- c("F11DP", "F10EE", "F12PA") #BMI indicators are of interest
  for(r1 in 1:nrow(mi.train)){ #outer loop through all MI table rows
    lhsterm <- mi.train[r1,1] #save predicted value of the path to an object
    rhsterm <- mi.train[r1,3]#save predictor of the path to an object
    epcterm <- mi.train[r1,5]#save EPC to an object
    value.left <- logic[lhsterm]#extract value of predicted term from "logic"
    value.right <- logic[rhsterm]#extract value of predictor term from "logic"
    free.parameter.temp <- paste(lhsterm, 
                                 "~", 
                                 rhsterm) #save parameter in proper format
    if (lhsterm %in% interest & epcterm*value.left*value.right>0){
      break   #when predicted terms are from "interest", it's of study focus
    } }       #when predictor*predicted*EPC>0, it's logical
    if (lhsterm %in% interest & epcterm*value.left*value.right>0){
              #examine if the parameter saved is really what we want
      free.parameter.final <- free.parameter.temp  
    }else{         #if it is not what we want, start the inner loop
      for(r2 in 1:nrow(mi.train)){  #inner loop starts when outer loop fail
        lhsterm <- mi.train[r2,1]   #to find any competent parameter
        rhsterm <- mi.train[r2,3]   #these several lines are same idea 
        epcterm <- mi.train[r2,5]   #with lines in outer loop
        value.left <- logic[lhsterm]
        value.right <- logic[rhsterm]
        free.parameter.temp <- paste(lhsterm, "~", rhsterm) 
        if(epcterm*value.left*value.right>0){ #this time only examine if logical
          break} 
        }
      free.parameter.final <- free.parameter.temp #save the final parameter 
    } 
  print(free.parameter.final) #print it
}
```

### Execute loop A (select path via statistics and minimal logics)

This loop is to automate 20 rounds of host-hoc analyses 60 times, each time on a different train/test sets, with regression path selected by sheer statistics. For more information, see the description above.

```{r, eval=F}
initial_model <- '
# Factors:
 F1ROLA =~ ROLEA1 + ROLEA2 + DEC2
 F2ROLC =~ ROLEC1 + ROLEC2
 F3WORK =~ WORK1 + WORK2
 F4CLIM =~ CCLIM1 + CCLIM2 + CCLIM3 + CCLIM4
 F5DEC =~ DEC1 + DEC2
 F6SSUP =~ SSUP1 + SSUP2 + DEC2
 F7PSUP =~ PSUP1 + PSUP2
 F8SELF =~ SELF1 + SELF2 + SELF3
 F9ELC =~ ELC1 + ELC2 + ELC3 + ELC4 + ELC5
 F10EE =~ EE1 + EE2 + EE3
 F11DP =~ DP1 + DP2
 F12PA =~ PA1 + PA2 + PA3
# Regressions:
 F8SELF ~ F5DEC + F6SSUP + F7PSUP
 F9ELC ~ F5DEC
 F10EE ~ F2ROLC + F3WORK + F4CLIM
 F11DP ~ F2ROLC + F10EE
 F12PA ~ F1ROLA + F8SELF + F9ELC + F10EE + F11DP
'
#function starts here
logic <- c(-1,-1,-1,1,1,1,1,1,1,-1,-1,1)
names(logic) <- c("F1ROLA","F2ROLC","F3WORK","F4CLIM",
                  "F5DEC","F6SSUP","F7PSUP","F8SELF",
                  "F9ELC","F10EE","F11DP","F12PA")

model.table <- matrix(NA, ncol = 15, nrow = 1) |> as.data.frame()

colnames(model.table)<- c("chi.square", "df", "cfi", "tli", "rmsea", "srmr", "csf",#1-7
                     "mi.largest", #8 old model
                     "n.anomaly", #9 old model
                     "new.para.sig",#10 next model
                     "chi.diff",#11
                     "free.parameter",#12
                     "modification.n",#13
                     "train/test",#14
                     "random.n")#15




for (m in 1:60){
  table <- matrix(NA, ncol = 15, nrow = 40) |> as.data.frame()
  colnames(table) <- c("chi.square", "df", "cfi", "tli", "rmsea", "srmr", "csf",#1-7
                     "mi.largest", #8 old model
                     "n.anomaly", #9 old model
                     "new.para.sig",#10 next model
                     "chi.diff",#11
                     "free.parameter",#12
                     "modification.n",#13
                     "train/test",#14
                     "random.n")#15
  table[1:40, 14] <- rep(c("train", "test"), 20)
  set.seed(m)
  nr <-  nrow(mbi)
  #separate train and test data sets
  ind <- sample(1:nr, size = nr*0.5)
  train <- mbi[ind,]
  test <- mbi[-ind,]
  model <- initial_model
  n_of_mod <- seq(1,by=2, len=20)
  
for (a in n_of_mod){
  tryCatch({
  table[a, 13] <- (a+1)/2
  table[a+1,13] <- (a+1)/2
  #estimate
  model.train <- sem(model, data = train, estimator = "MLM", mimic = "Mplus")
  model.test <- sem(model, data = test, estimator = "MLM", mimic = "Mplus")
  #pass fit indices of train set to table
  fit.train <- fitMeasures(model.train, c("chisq.scaled","df.scaled","cfi.scaled","tli.scaled",
                              "rmsea.scaled","srmr_bentler","chisq.scaling.factor"))
  table[a,1:7] <- fit.train
  #pass fit indices of test set to table
  fit.test <- fitMeasures(model.test, c("chisq.scaled","df.scaled","cfi.scaled","tli.scaled",
                              "rmsea.scaled","srmr_bentler","chisq.scaling.factor"))
  table[a+1,1:7] <- fit.test
  
  #parameter estimates (for alarming parameters)
  #pass proportion of p>0.05 parameters into table, column 10
  concern.train<- parameterEstimates(model.train, standardized=TRUE) |>  
    filter(op == "~") |>
    select(pvalue)
  concern.test<- parameterEstimates(model.test, standardized=TRUE) |>  
    filter(op == "~") |>
    select(pvalue)
  table[a,9] <- length(concern.train$pvalue[concern.train$pvalue>0.05])/length(concern.train$pvalue)
  table[a+1,9] <- length(concern.test$pvalue[concern.test$pvalue>0.05])/length(concern.test$pvalue)
  #modification indices
  mi.train <- modindices(model.train,standardized = TRUE, sort. = TRUE)
  mi.train <- mi.train |> filter(op == "~") |>  
    filter (!rhs %in% c("F8SELF","F9ELC","F10EE","F11DP","F12PA"))  
  #extract the best parameter to set free
  free.parameter <- mi.train |> select(lhs, op, rhs)
  free.parameter <- free.parameter[1,] |> unlist() |> unname() |> paste(collapse = "")
  table[a,12] <- free.parameter
  table[a+1,12] <- free.parameter
  table[a,8] <- mi.train[1,4]
  
  ##########

  #next model
  model <- paste(model, "\n",free.parameter)
  #estimate
  model.train2 <- sem(model, data = train, estimator = "MLM", mimic = "Mplus")
  model.test2 <- sem(model, data = test, estimator = "MLM", mimic = "Mplus") 
  #chi.diff
  table[a, 11] <- chi.diff(model.train, model.train2)
  table[a+1, 11] <- chi.diff(model.test, model.test2)
  #if sig.
  sig.new.para.train<- parameterEstimates(model.train2, standardized=TRUE) |>  
    filter(op == "~") |>
    tail(1) |> 
    select(pvalue)
  sig.new.para.test<- parameterEstimates(model.test2, standardized=TRUE) |>  
    filter(op == "~") |>
    tail(1) |> 
    select(pvalue)
  #pass if new parameter is sig to table 
  table[a,10] <- sig.new.para.train[1,1]<0.05#train
  table[a+1,10] <- sig.new.para.test[1,1]<0.05#test
  }, error = function(e){})
  
} 
  table[1:40,15] <- m
  model.table <- rbind(model.table, table)
}

```


### Execute loop B (select path via rule-based criteria that approximate Byrne's logic)

This loop is to automate 20 rounds of host-hoc analyses for 60 times, each time on a different train/test sets, with regression path selected by rule-based criteria. For more information, see the description above.

```{r, eval=F}
initial_model <- '
# Factors:
 F1ROLA =~ ROLEA1 + ROLEA2 + DEC2
 F2ROLC =~ ROLEC1 + ROLEC2
 F3WORK =~ WORK1 + WORK2
 F4CLIM =~ CCLIM1 + CCLIM2 + CCLIM3 + CCLIM4
 F5DEC =~ DEC1 + DEC2
 F6SSUP =~ SSUP1 + SSUP2 + DEC2
 F7PSUP =~ PSUP1 + PSUP2
 F8SELF =~ SELF1 + SELF2 + SELF3
 F9ELC =~ ELC1 + ELC2 + ELC3 + ELC4 + ELC5
 F10EE =~ EE1 + EE2 + EE3
 F11DP =~ DP1 + DP2
 F12PA =~ PA1 + PA2 + PA3
# Regressions:
 F8SELF ~ F5DEC + F6SSUP + F7PSUP
 F9ELC ~ F5DEC
 F10EE ~ F2ROLC + F3WORK + F4CLIM
 F11DP ~ F2ROLC + F10EE
 F12PA ~ F1ROLA + F8SELF + F9ELC + F10EE + F11DP
'
#function starts here
logic <- c(-1,-1,-1,1,1,1,1,1,1,-1,-1,1)
names(logic) <- c("F1ROLA","F2ROLC","F3WORK","F4CLIM",
                  "F5DEC","F6SSUP","F7PSUP","F8SELF",
                  "F9ELC","F10EE","F11DP","F12PA")

model.table <- matrix(NA, ncol = 15, nrow = 1) |> as.data.frame()

colnames(model.table)<- c("chi.square", "df", "cfi", "tli", "rmsea", "srmr", "csf",#1-7
                     "mi.largest", #8 old model
                     "n.anomaly", #9 old model
                     "new.para.sig",#10 next model
                     "chi.diff",#11
                     "free.parameter",#12
                     "modification.n",#13
                     "train/test",#14
                     "random.n")#15




for (m in 1:60){
  table <- matrix(NA, ncol = 15, nrow = 40) |> as.data.frame()
  colnames(table) <- c("chi.square", "df", "cfi", "tli", "rmsea", "srmr", "csf",#1-7
                     "mi.largest", #8 old model
                     "n.anomaly", #9 old model
                     "new.para.sig",#10 next model
                     "chi.diff",#11
                     "free.parameter",#12
                     "modification.n",#13
                     "train/test",#14
                     "random.n")#15
  table[1:40, 14] <- rep(c("train", "test"), 20)
  set.seed(m)
  nr <-  nrow(mbi)
  #separate train and test data sets
  ind <- sample(1:nr, size = nr*0.5)
  train <- mbi[ind,]
  test <- mbi[-ind,]
  model <- initial_model
  n_of_mod <- seq(1,by=2, len=20)
  
for (a in n_of_mod){
  tryCatch({
  table[a, 13] <- (a+1)/2
  table[a+1,13] <- (a+1)/2
  #estimate
  model.train <- sem(model, data = train, estimator = "MLM", mimic = "Mplus")
  model.test <- sem(model, data = test, estimator = "MLM", mimic = "Mplus")
  #pass fit indices of train set to table
  fit.train <- fitMeasures(model.train, c("chisq.scaled","df.scaled","cfi.scaled","tli.scaled",
                              "rmsea.scaled","srmr_bentler","chisq.scaling.factor"))
  table[a,1:7] <- fit.train
  #pass fit indices of test set to table
  fit.test <- fitMeasures(model.test, c("chisq.scaled","df.scaled","cfi.scaled","tli.scaled",
                              "rmsea.scaled","srmr_bentler","chisq.scaling.factor"))
  table[a+1,1:7] <- fit.test
  
  #parameter estimates (for alarming parameters)
  #pass proportion of p>0.05 parameters into table, column 10
  concern.train<- parameterEstimates(model.train, standardized=TRUE) |>  
    filter(op == "~") |>
    select(pvalue)
  concern.test<- parameterEstimates(model.test, standardized=TRUE) |>  
    filter(op == "~") |>
    select(pvalue)
  table[a,9] <- length(concern.train$pvalue[concern.train$pvalue>0.05])/length(concern.train$pvalue)
  table[a+1,9] <- length(concern.test$pvalue[concern.test$pvalue>0.05])/length(concern.test$pvalue)
  #modification indices
  mi.train <- modindices(model.train,standardized = TRUE, sort. = TRUE)
  mi.train <- mi.train |> filter(op == "~") |>  
    filter (!rhs %in% c("F8SELF","F9ELC","F10EE","F11DP","F12PA"))  
  #extract the best parameter to set free
  #####free.parameter <- mi.train |> select(lhs, op, rhs)
  #####free.parameter <- free.parameter[1,] |> unlist() |> unname() |> paste(collapse = "")
  free.parameter <- find.path(mi.train)
  table[a,12] <- free.parameter
  table[a+1,12] <- free.parameter
  table[a,8] <- mi.train[1,4]
  
  ##########

  #next model
  model <- paste(model, "\n",free.parameter)
  #estimate
  model.train2 <- sem(model, data = train, estimator = "MLM", mimic = "Mplus")
  model.test2 <- sem(model, data = test, estimator = "MLM", mimic = "Mplus") 
  #chi.diff
  table[a, 11] <- chi.diff(model.train, model.train2)
  table[a+1, 11] <- chi.diff(model.test, model.test2)
  #if sig.
  sig.new.para.train<- parameterEstimates(model.train2, standardized=TRUE) |>  
    filter(op == "~") |>
    tail(1) |> 
    select(pvalue)
  sig.new.para.test<- parameterEstimates(model.test2, standardized=TRUE) |>  
    filter(op == "~") |>
    tail(1) |> 
    select(pvalue)
  #pass if new parameter is sig to table 
  table[a,10] <- sig.new.para.train[1,1]<0.05#train
  table[a+1,10] <- sig.new.para.test[1,1]<0.05#test
  }, error = function(e){})
  
} 
  table[1:40,15] <- m
  model.table <- rbind(model.table, table)
}
library(DT) 
model.table |> datatable()
```

```{r}
modeltable.sta <- readRDS(file = "statmodeltable20.rds")
modeltable.rule <- readRDS(file = "rulebasedmodeltabler20.rds")
modeltable.sta<- modeltable.sta[-1,]
modeltable.rule <- modeltable.rule[-1,]
```

### Visualize the result

```{r}
sta <- modeltable.sta |> 
  group_by(modification.n, `train/test`) |> 
  summarise(mean.chi.square = mean(chi.square, na.rm = T), 
            sd.chi.square = sd(chi.square, na.rm = T),
            mean.chi.square.diff =mean(chi.diff, na.rm = T),
            sd.chi.square.diff = sd(chi.diff, na.rm = T),
            n.chi.square=length(!is.na(chi.square)),
            n.chi.square.diff=length(!is.na(chi.diff)),
            mean.anomaly =mean(n.anomaly, na.rm = T),
            sd.anomaly = sd(n.anomaly, na.rm = T),
            n.anomaly=length(!is.na(n.anomaly)),
            mean.cfi =  mean(cfi, na.rm = T),
            sd.cfi = sd(cfi, na.rm = T),
            n.cfi = length (!is.na(cfi)),
            mean.tli =  mean(tli, na.rm = T),
            sd.tli = sd(tli, na.rm = T),
            n.tli = length (!is.na(tli)),
            mean.rmsea = mean(rmsea, na.rm = T),
            sd.rmsea = sd(rmsea, na.rm = T),
            n.rmsea = length(!is.na(rmsea))
            )
```      

```{r}
rule <- modeltable.rule |> 
  group_by(modification.n, `train/test`) |> 
  summarise(mean.chi.square = mean(chi.square, na.rm = T), 
            sd.chi.square = sd(chi.square, na.rm = T),
            mean.chi.square.diff =mean(chi.diff, na.rm = T),
            sd.chi.square.diff = sd(chi.diff, na.rm = T),
            n.chi.square=length(!is.na(chi.square)),
            n.chi.square.diff=length(!is.na(chi.diff)),
            mean.anomaly =mean(n.anomaly, na.rm = T),
            sd.anomaly = sd(n.anomaly, na.rm = T),
            n.anomaly=length(!is.na(n.anomaly)),
            mean.cfi =  mean(cfi, na.rm = T),
            sd.cfi = sd(cfi, na.rm = T),
            n.cfi = length (!is.na(cfi)),
            mean.tli =  mean(tli, na.rm = T),
            sd.tli = sd(tli, na.rm = T),
            n.tli = length (!is.na(tli)),
            mean.rmsea = mean(rmsea, na.rm = T),
            sd.rmsea = sd(rmsea, na.rm = T),
            n.rmsea = length(!is.na(rmsea))
            )
```

```{r}
sta$select.type <- "statistics"
rule$select.type <- "rule"
```

```{r}
combine <- rbind(sta,rule)
combine.fa <- combine |>  #fa is for analysis
  select(train_test = 'train/test', select.type, modification.n, everything()) |> 
  group_by("train/test", "select.type") |> 
  mutate(group =  paste0(train_test,
                         "_",
                         select.type)) |> 
  select(group, everything()) |> 
  na.omit()

```


(1) Chi square statistics 

The smaller the chi square statistics, the better, because we want the p value >0.05 to support model fit.


```{r, fig.width= 12, fig.height=9}
a <- combine.fa |>  
  filter(group %in% c("train_statistics", "test_statistics")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.chi.square,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.chi.square-1.96*sd.chi.square/sqrt(n.chi.square), 
                  ymax = 
                    mean.chi.square+1.96*sd.chi.square/sqrt(n.chi.square),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.8, 0.8),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "Chi Square Statistics",
       title = "Train set vs Test set by statistical path selection") +
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))

b <- combine.fa |>  
  filter(group %in% c("train_rule", "test_rule")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.chi.square,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.chi.square-1.96*sd.chi.square/sqrt(n.chi.square), 
                  ymax = 
                    mean.chi.square+1.96*sd.chi.square/sqrt(n.chi.square),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.2, 0.8),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "Chi Square Statistics",
       title = "Train set vs Test set by rule-based path selection") +
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))

c <- combine.fa |>  
  filter(group %in% c("train_statistics", "train_rule")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.chi.square,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.chi.square-1.96*sd.chi.square/sqrt(n.chi.square), 
                  ymax = 
                    mean.chi.square+1.96*sd.chi.square/sqrt(n.chi.square),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.8, 0.8),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "Chi Square Statistics",
       title = "Train set by rule-based path selection vs by statistical selection")+
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))

d <- combine.fa |>  
  filter(group %in% c("test_statistics", "test_rule")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.chi.square,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.chi.square-1.96*sd.chi.square/sqrt(n.chi.square), 
                  ymax = 
                    mean.chi.square+1.96*sd.chi.square/sqrt(n.chi.square),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.2, 0.8),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "Chi Square Statistics",
       title = "Test set by rule-based path selection vs by statistical selection") +
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))


library(patchwork)
a/b|c/d
```

Before interpreting the results, please note that rule-based selection is an approximation of Byrne's theory-based path selection; and statistic-based selection always chooses the regression path based on the largest MI, except for some minimal logic that MBI indicators will never leads to indicators outside BMI. 

Looking at the size of chi square decrease singly, we can see that statistics-based method outperformed rule-based method by around 50 chi-square value. This difference was not significant between train sets (see picture upper right corner). Yet it was significant for test sets (picture lower right corner). However, there are more to interpret than numbers.

Note that even though I set an 20-round modification, the processes for all four data sets stopped themselves long before that, often at 12~14 models into post-hoc analyses, suggesting at those points there were basically not any more paths to set free. Interestingly, rule-based selection (train and test sets) stopped at the 12th models, whereas statistics-based selection (train and test sets) stopped at the 14th models. The explanation could be that rule-based selection actually works better in a way of exhausting the candidate regression paths faster. This makes practical sense since rule-based selection balanced the statistics and some implicit mechanism that people's perception follows, and hence fit the data better albeit the benefits could only manifest itself after several models into post-hoc analyses. However, the chi squares decrease much milder (or even increase a bit) at 3~5 models into post-hocs. 

We want the Chi square smaller since we want p value >0.05. It is so clear from the picture that both the rule-based and statistics-based test data sets (green and red) had elbow effect at 9~10 times into modifications. After which, the chi square value went up dramatically (getting bad). In contrast, both train sets were roughly stable throughout the modifications. Put together, we could arrive at the idea that after 8~9 models into post-hocs, if we keep freeing more regression paths, we will overfit the data. 

(2) CFI

CFI always <1. The closer the CFI to 1, the better.

```{r, fig.width= 12, fig.height=9}
a <- combine.fa |>  
  filter(group %in% c("train_statistics", "test_statistics")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.cfi,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.cfi-1.96*sd.cfi/sqrt(n.cfi), 
                  ymax = 
                    mean.cfi+1.96*sd.cfi/sqrt(n.cfi),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.8, 0.2),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "CFI",
       title = "Train set vs Test set by statistical path selection") +
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))

b <- combine.fa |>  
  filter(group %in% c("train_rule", "test_rule")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.cfi,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.cfi-1.96*sd.cfi/sqrt(n.cfi), 
                  ymax = 
                    mean.cfi+1.96*sd.cfi/sqrt(n.cfi),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.2, 0.2),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "CFI",
       title = "Train set vs Test set by rule-based path selection") +
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))

c <- combine.fa |>  
  filter(group %in% c("train_statistics", "train_rule")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.cfi,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.cfi-1.96*sd.cfi/sqrt(n.cfi), 
                  ymax = 
                    mean.cfi+1.96*sd.cfi/sqrt(n.cfi),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.8, 0.2),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "CFI",
       title = "Train set by rule-based path selection vs by statistical selection")+
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))

d <- combine.fa |>  
  filter(group %in% c("test_statistics", "test_rule")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.cfi,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.cfi-1.96*sd.cfi/sqrt(n.cfi), 
                  ymax = 
                    mean.cfi+1.96*sd.cfi/sqrt(n.cfi),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.2, 0.2),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "CFI",
       title = "Test set by rule-based path selection vs by statistical selection")+
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))


a/b|c/d
```

CFI are expected to be over 0.95. The closer to 1, the better. We see that elbow effect again in test sets. It happened again at around 9~10 models into the post-hocs. It suggests for the current data set we had better to stop adding new regression paths before 9 models, or else we would over-fit the data.

Again, if we only examine numbers, statistics-based selection outperforms rule-based selection by around 0.005 (it's so small, but significant). Note that both rule-based and statistics-based CFI passed the 0.95 requirement after 3~4 models into the post-hocs. And again, the rule-based procedure exhausted the possible regression path to be set free earlier than statistics-based selection by 2 models. 

(3) RMSEA

The smaller, the better.

```{r, fig.width= 12, fig.height=9}
a <- combine.fa |>  
  filter(group %in% c("train_statistics", "test_statistics")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.rmsea,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.rmsea-1.96*sd.rmsea/sqrt(n.rmsea), 
                  ymax = 
                    mean.rmsea+1.96*sd.rmsea/sqrt(n.rmsea),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.8, 0.8),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "RMSEA",
       title = "Train set vs Test set by statistical path selection") +
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))

b <- combine.fa |>  
  filter(group %in% c("train_rule", "test_rule")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.rmsea,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.rmsea-1.96*sd.rmsea/sqrt(n.rmsea), 
                  ymax = 
                    mean.rmsea+1.96*sd.rmsea/sqrt(n.rmsea),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.2, 0.8),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "RMSEA",
       title = "Train set vs Test set by rule-based path selection") +
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))

c <- combine.fa |>  
  filter(group %in% c("train_statistics", "train_rule")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.rmsea,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.rmsea-1.96*sd.rmsea/sqrt(n.rmsea), 
                  ymax = 
                    mean.rmsea+1.96*sd.rmsea/sqrt(n.rmsea),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.8, 0.8),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "RMSEA",
       title = "Train set by rule-based path selection vs by statistical selection")+
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))

d <- combine.fa |>  
  filter(group %in% c("test_statistics", "test_rule")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.rmsea,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.rmsea-1.96*sd.rmsea/sqrt(n.rmsea), 
                  ymax = 
                    mean.rmsea+1.96*sd.rmsea/sqrt(n.rmsea),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.2, 0.8),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "RMSEA",
       title = "Test set by rule-based path selection vs by statistical selection")+
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))


a/b|c/d
```

The results of RMSEA follows the pattern of the previous two indicators. In short, test sets had elbow effect; statistics-based selection had nicer (smaller) numbers, but rule-based selection also got small enough to meet the requirement. Moreover, rule-based selection exhausted the possible regression paths faster. 

(2) proportion of anomaly terms

This is the proportion of insignificant regression paths. The smaller, the better.

```{r, fig.width= 12, fig.height=9}
a <- combine.fa |>  
  filter(group %in% c("train_statistics", "test_statistics")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.anomaly,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.anomaly-1.96*sd.anomaly/sqrt(n.anomaly), 
                  ymax = 
                    mean.anomaly+1.96*sd.anomaly/sqrt(n.anomaly),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.2, 0.8),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "Proportion of insignificant regression paths",
       title = "Train set vs Test set by statistical path selection") +
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))

b <- combine.fa |>  
  filter(group %in% c("train_rule", "test_rule")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.anomaly,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.anomaly-1.96*sd.anomaly/sqrt(n.anomaly), 
                  ymax = 
                    mean.anomaly+1.96*sd.anomaly/sqrt(n.anomaly),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.2, 0.8),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "Proportion of insignificant regression paths",
       title = "Train set vs Test set by rule-based path selection") +
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))

c <- combine.fa |>  
  filter(group %in% c("train_statistics", "train_rule")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.anomaly,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.anomaly-1.96*sd.anomaly/sqrt(n.anomaly), 
                  ymax = 
                    mean.anomaly+1.96*sd.anomaly/sqrt(n.anomaly),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.2, 0.8),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "Proportion of insignificant regression paths",
       title = "Train set by rule-based path selection vs by statistical selection")+
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))

d <- combine.fa |>  
  filter(group %in% c("test_statistics", "test_rule")) |> 
  ggplot(aes(x = factor(modification.n), 
             y = mean.anomaly,  group = group, linetype =group)) +
    geom_line()+
  geom_point(type =2, size = 0.1)+
  geom_ribbon(aes(ymin = 
                    mean.anomaly-1.96*sd.anomaly/sqrt(n.anomaly), 
                  ymax = 
                    mean.anomaly+1.96*sd.anomaly/sqrt(n.anomaly),  
                  fill = group), alpha =0.20,
                position = position_dodge(0.1))+
  theme(panel.background = element_rect(fill = "white",
                                        color = "black"),
        legend.position = c(0.2, 0.8),
        plot.title = element_text(size = 13))+
  labs(x = "Times into post-hoc analysis",
       y = "Proportion of insignificant regression paths",
       title = "Test set by rule-based path selection vs by statistical selection")+
   scale_fill_manual(values = c(test_statistics="brown",
                                train_statistics="blue",
                                 test_rule ="green",
                                  train_rule ="yellow"))


a/b|c/d
```

The results is very interesting, rule-based selection saw its smallest proportion of non-significant paths at 3 models into post-hocs; and the pattern of proportion changes were consistent between train and test before 11 models. However, the stat-based selection saw its smallest proportion at 4 and 5 models (slower) and the pattern of proportion changes were inconsistent between train and test sets throughout. However, the change pattern between test sets of two selection methods were consistent (see pic lower right). Overall, the stat-based selection outperforms rule-based method a bit by 5% and the difference is significant.


I have collected more indices/indicators than the four I have shown. However, they basically exhibit similar pattern with this most representative four. 

In summary, the post-hoc analyses of BMI data set, no matter by which way we select regression path to set free, should stop at 8~9 models into post-hocs, since after that we start to over-fit the data. The statistics-based path selection outperforms the rule-based selection (Bynre's selection method) by statistical numbers, but only marginally. And the rule-based selection outperforms the stat-based selection by exhausting the possible regression paths faster. 

There should be one final step that I build one more model for each analyses by deleting the insignificant paths; due to the time limitation, I will keep this step for the further. 

# Summary

In the exercise, I practiced post-hoc analysis of full SEM with teacher burnout data set. Over the process, I established 8 models step by step and compared their model fit indices, regression path estimates, and selected alarming variances;

Over the exercises, I mostly followed the step of the slides and Byrne's book. However, in models 7 and 8, I started to go on a different tract, leading to a different final model. 

I learnt something that I would definitely misunderstand or mistake as I know if I hadn't done this exercise. Like, when removing factors for the final model, we should keep the factor loading (measurement model) of the deleted factors in the model; 

There are some visual effects I want to realize using Semplot that I finally failed to, such as showing the edge as broken arrows. Need to dig more next week. 

Calculating degrees of freedom is very chanlleging. I understand every bit of it when the process is shown to me. However, when I need to calculate without any external aide, I always forgot this or that. 

I found SEM is a beautiful blending of statistics and mindful sense-making. Indeed, they always go together. Yet, not often so well balanced as they are in SEM, for the scope of statistics I am familiar with. 





xie
